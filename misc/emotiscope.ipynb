{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nworb999/emotiscope/blob/main/emotiscope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg01WmrKT_j3"
   },
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Cc5kXa335Vu",
    "outputId": "915024f3-f4ac-4a56-d38f-8e0412ad2bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: transformers in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (4.41.1)\n",
      "Requirement already satisfied: scipy in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (1.13.1)\n",
      "Requirement already satisfied: ftfy in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (6.2.0)\n",
      "Requirement already satisfied: accelerate in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (0.30.1)\n",
      "Requirement already satisfied: torch in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: np in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (1.0.2)\n",
      "Requirement already satisfied: opencv-python in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: streamlit in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (1.35.0)\n",
      "Requirement already satisfied: streamlit-webrtc in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (0.47.7)\n",
      "Requirement already satisfied: av in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (11.0.0)\n",
      "Requirement already satisfied: pyngrok in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (7.1.6)\n",
      "Requirement already satisfied: twilio in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (9.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (7.1.0)\n",
      "Requirement already satisfied: filelock in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (0.23.1)\n",
      "Requirement already satisfied: numpy in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (0.4.3)\n",
      "Requirement already satisfied: Pillow in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from diffusers) (10.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: psutil in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: sympy in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit) (6.4)\n",
      "Requirement already satisfied: aiortc<2.0.0,>=1.4.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from streamlit-webrtc) (1.8.0)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from twilio) (2.8.0)\n",
      "Requirement already satisfied: aiohttp>=3.8.4 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from twilio) (3.9.5)\n",
      "Requirement already satisfied: aiohttp-retry>=2.8.3 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from twilio) (2.8.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiohttp>=3.8.4->twilio) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiohttp>=3.8.4->twilio) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiohttp>=3.8.4->twilio) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiohttp>=3.8.4->twilio) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiohttp>=3.8.4->twilio) (1.9.4)\n",
      "Requirement already satisfied: aioice<1.0.0,>=0.9.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (0.9.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=42.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (42.0.7)\n",
      "Requirement already satisfied: google-crc32c>=1.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (1.5.0)\n",
      "Requirement already satisfied: pyee>=9.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (11.1.0)\n",
      "Requirement already satisfied: pylibsrtp>=0.10.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (0.10.0)\n",
      "Requirement already satisfied: pyopenssl>=24.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (24.1.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.22.0)\n",
      "Requirement already satisfied: toolz in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from requests->diffusers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from requests->diffusers) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.19.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (2.6.1)\n",
      "Requirement already satisfied: ifaddr>=0.2.0 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (0.2.0)\n",
      "Requirement already satisfied: pycparser in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from cffi>=1.0.0->aiortc<2.0.0,>=1.4.0->streamlit-webrtc) (2.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nworb/Documents/code/emotiscope/moody-env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers scipy ftfy accelerate torch torchvision np opencv-python streamlit streamlit-webrtc av pyngrok twilio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wu-C63erGuw"
   },
   "source": [
    "# Hugging Face Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Au_xHFa_19o",
    "outputId": "9af8a763-07e6-4a94-d91e-99528b7d138e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your API token: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Securely input the API token\n",
    "api_token = getpass.getpass(\"Enter your API token:\")\n",
    "os.environ[\"HF_TOKEN\"] = api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Pd0m4JU4Uyx-"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hlA7jukuIv4"
   },
   "source": [
    "# Facial Expression Recognition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WsNmAw7_vmEP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m image_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m127\u001b[39m, \u001b[38;5;241m127\u001b[39m, \u001b[38;5;241m127\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv2\n",
    "import time as time\n",
    "\n",
    "image_mean = np.array([127, 127, 127])\n",
    "image_std = 128.0\n",
    "iou_threshold = 0.3\n",
    "center_variance = 0.1\n",
    "size_variance = 0.2\n",
    "min_boxes = [\n",
    "    [10.0, 16.0, 24.0],\n",
    "    [32.0, 48.0],\n",
    "    [64.0, 96.0],\n",
    "    [128.0, 192.0, 256.0]\n",
    "]\n",
    "strides = [8.0, 16.0, 32.0, 64.0]\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5kTIbUayQTy"
   },
   "outputs": [],
   "source": [
    "def generate_priors(feature_map_list, shrinkage_list, image_size, min_boxes):\n",
    "    priors = []\n",
    "    for index in range(0, len(feature_map_list[0])):\n",
    "        scale_w = image_size[0] / shrinkage_list[0][index]\n",
    "        scale_h = image_size[1] / shrinkage_list[1][index]\n",
    "        for j in range(0, feature_map_list[1][index]):\n",
    "            for i in range(0, feature_map_list[0][index]):\n",
    "                x_center = (i + 0.5) / scale_w\n",
    "                y_center = (j + 0.5) / scale_h\n",
    "\n",
    "                for min_box in min_boxes[index]:\n",
    "                    w = min_box / image_size[0]\n",
    "                    h = min_box / image_size[1]\n",
    "                    priors.append([\n",
    "                        x_center,\n",
    "                        y_center,\n",
    "                        w,\n",
    "                        h\n",
    "                    ])\n",
    "    print(\"priors nums:{}\".format(len(priors)))\n",
    "    return np.clip(priors, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxT-PFnbxQKZ"
   },
   "outputs": [],
   "source": [
    "def define_img_size(image_size):\n",
    "    shrinkage_list = []\n",
    "    feature_map_w_h_list = []\n",
    "    for size in image_size:\n",
    "        feature_map = [int(math.ceil(size / stride)) for stride in strides]\n",
    "        feature_map_w_h_list.append(feature_map)\n",
    "\n",
    "    for i in range(0, len(image_size)):\n",
    "        shrinkage_list.append(strides)\n",
    "    priors = generate_priors(\n",
    "        feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
    "    )\n",
    "    return priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0f9Bb3vzG9O"
   },
   "outputs": [],
   "source": [
    "def area_of(left_top, right_bottom):\n",
    "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "    return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "    return overlap_area / (area0 + area1 - overlap_area + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gsb0kCglzCSw"
   },
   "outputs": [],
   "source": [
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "    scores = box_scores[:, -1]\n",
    "    boxes = box_scores[:, :-1]\n",
    "    picked = []\n",
    "    indexes = np.argsort(scores)\n",
    "    indexes = indexes[-candidate_size:]\n",
    "    while len(indexes) > 0:\n",
    "        current = indexes[-1]\n",
    "        picked.append(current)\n",
    "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "            break\n",
    "        current_box = boxes[current, :]\n",
    "        indexes = indexes[:-1]\n",
    "        rest_boxes = boxes[indexes, :]\n",
    "        iou = iou_of(\n",
    "            rest_boxes,\n",
    "            np.expand_dims(current_box, axis=0),\n",
    "        )\n",
    "        indexes = indexes[iou <= iou_threshold]\n",
    "    return box_scores[picked, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aoz9WhagzIlr"
   },
   "outputs": [],
   "source": [
    "def predict(\n",
    "    width,\n",
    "    height,\n",
    "    confidences,\n",
    "    boxes,\n",
    "    prob_threshold,\n",
    "    iou_threshold=0.3,\n",
    "    top_k=-1\n",
    "):\n",
    "    boxes = boxes[0]\n",
    "    confidences = confidences[0]\n",
    "    picked_box_probs = []\n",
    "    picked_labels = []\n",
    "    for class_index in range(1, confidences.shape[1]):\n",
    "        probs = confidences[:, class_index]\n",
    "        mask = probs > prob_threshold\n",
    "        probs = probs[mask]\n",
    "        if probs.shape[0] == 0:\n",
    "            continue\n",
    "        subset_boxes = boxes[mask, :]\n",
    "        box_probs = np.concatenate(\n",
    "            [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
    "        )\n",
    "        box_probs = hard_nms(box_probs,\n",
    "                             iou_threshold=iou_threshold,\n",
    "                             top_k=top_k,\n",
    "                             )\n",
    "        picked_box_probs.append(box_probs)\n",
    "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "    if not picked_box_probs:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    picked_box_probs = np.concatenate(picked_box_probs)\n",
    "    picked_box_probs[:, 0] *= width\n",
    "    picked_box_probs[:, 1] *= height\n",
    "    picked_box_probs[:, 2] *= width\n",
    "    picked_box_probs[:, 3] *= height\n",
    "    return (\n",
    "        picked_box_probs[:, :4].astype(np.int32),\n",
    "        np.array(picked_labels),\n",
    "        picked_box_probs[:, 4]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCCHOR7cz_w6"
   },
   "outputs": [],
   "source": [
    "def convert_locations_to_boxes(locations, priors, center_variance,\n",
    "                               size_variance):\n",
    "    if len(priors.shape) + 1 == len(locations.shape):\n",
    "        priors = np.expand_dims(priors, 0)\n",
    "    return np.concatenate([\n",
    "        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
    "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "    ], axis=len(locations.shape) - 1)\n",
    "\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "    return np.concatenate(\n",
    "        [locations[..., :2] - locations[..., 2:] / 2,\n",
    "         locations[..., :2] + locations[..., 2:] / 2],\n",
    "        len(locations.shape) - 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7wkzsfP8sp2"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "# This code is based on https://github.com/streamlit/demo-self-driving/blob/230245391f2dda0cb464008195a470751c01770b/streamlit_app.py#L48  # noqa: E501\n",
    "# and I, Emma, ripped this from https://huggingface.co/spaces/sadimanna/face-emotion-recognition/\n",
    "\n",
    "def download_file(url, download_to: Path, expected_size=None):\n",
    "    # Don't download the file twice.\n",
    "    # (If possible, verify the download using the file length.)\n",
    "    if download_to.exists():\n",
    "        if expected_size:\n",
    "            if download_to.stat().st_size == expected_size:\n",
    "                return\n",
    "        else:\n",
    "            st.info(f\"{url} is already downloaded.\")\n",
    "            if not st.button(\"Download again?\"):\n",
    "                return\n",
    "\n",
    "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # These are handles to two visual elements to animate.\n",
    "    weights_warning, progress_bar = None, None\n",
    "    try:\n",
    "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
    "        progress_bar = st.progress(0)\n",
    "        with open(download_to, \"wb\") as output_file:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                length = int(response.info()[\"Content-Length\"])\n",
    "                counter = 0.0\n",
    "                MEGABYTES = 2.0 ** 20.0\n",
    "                while True:\n",
    "                    data = response.read(8192)\n",
    "                    if not data:\n",
    "                        break\n",
    "                    counter += len(data)\n",
    "                    output_file.write(data)\n",
    "\n",
    "                    # We perform animation by overwriting the elements.\n",
    "                    weights_warning.warning(\n",
    "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
    "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
    "                    )\n",
    "                    progress_bar.progress(min(counter / length, 1.0))\n",
    "    # Finally, we remove these visual elements by calling .empty().\n",
    "    finally:\n",
    "        if weights_warning is not None:\n",
    "            weights_warning.empty()\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifyIFtvm8ZF4"
   },
   "outputs": [],
   "source": [
    "HERE = Path.cwd()\n",
    "ROOT = HERE\n",
    "\n",
    "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
    "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
    "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
    "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
    "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
    "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt\"\n",
    "\n",
    "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
    "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
    "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH) #, expected_size=29353)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7MB03ztJZXp"
   },
   "source": [
    "# Live Camera Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqFBfGfu0iYI"
   },
   "outputs": [],
   "source": [
    "def FER_live_cam():\n",
    "    emotion_dict = {\n",
    "        0: 'neutral',\n",
    "        1: 'happiness',\n",
    "        2: 'surprise',\n",
    "        3: 'sadness',\n",
    "        4: 'anger',\n",
    "        5: 'disgust',\n",
    "        6: 'fear'\n",
    "    }\n",
    "\n",
    "    # cap = cv2.VideoCapture('video1.mp4')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    size = (frame_width, frame_height)\n",
    "    result = cv2.VideoWriter('result.avi',\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         10, size)\n",
    "\n",
    "    # Read ONNX model\n",
    "    # model = 'onnx_model.onnx'\n",
    "    model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "\n",
    "    # Read the Caffe face detector.\n",
    "    model_path = 'RFB-320/RFB-320.caffemodel'\n",
    "    proto_path = 'RFB-320/RFB-320.prototxt'\n",
    "    net = cv2.dnn.readNetFromCaffe(proto_path, model_path)\n",
    "    input_size = [320, 240]\n",
    "    width = input_size[0]\n",
    "    height = input_size[1]\n",
    "    priors = define_img_size(input_size)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            img_ori = frame\n",
    "            #print(\"frame size: \", frame.shape)\n",
    "            rect = cv2.resize(img_ori, (width, height))\n",
    "            rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
    "            net.setInput(cv2.dnn.blobFromImage(\n",
    "                rect, 1 / image_std, (width, height), 127)\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
    "            boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
    "            scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
    "            boxes = convert_locations_to_boxes(\n",
    "                boxes, priors, center_variance, size_variance\n",
    "            )\n",
    "            boxes = center_form_to_corner_form(boxes)\n",
    "            boxes, labels, probs = predict(\n",
    "                img_ori.shape[1],\n",
    "                img_ori.shape[0],\n",
    "                scores,\n",
    "                boxes,\n",
    "                threshold\n",
    "            )\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            for (x1, y1, x2, y2) in boxes:\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
    "                resize_frame = cv2.resize(\n",
    "                    gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
    "                )\n",
    "                resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
    "                model.setInput(resize_frame)\n",
    "                output = model.forward()\n",
    "                end_time = time.time()\n",
    "                fps = 1 / (end_time - start_time)\n",
    "                print(f\"FPS: {fps:.1f}\")\n",
    "                pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
    "                cv2.rectangle(\n",
    "                    img_ori,\n",
    "                    (x1, y1),\n",
    "                    (x2, y2),\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    pred,\n",
    "                    (x1, y1),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "\n",
    "            result.write(frame)\n",
    "\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    result.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "obOoWp7q01et",
    "outputId": "5e72861a-bbe1-452b-e242-b4feeffa62b0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Define the emotion dictionary\n",
    "emotion_dict = {\n",
    "    0: 'neutral', 1: 'happiness', 2: 'surprise', 3: 'sadness',\n",
    "    4: 'anger', 5: 'disgust', 6: 'fear'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "# Assuming you have already downloaded these models to your Colab environment\n",
    "model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "net = cv2.dnn.readNetFromCaffe('RFB-320/RFB-320.prototxt', 'RFB-320/RFB-320.caffemodel')\n",
    "\n",
    "# Define the image size for the face detector\n",
    "input_size = [320, 240]\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "    js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Read the image\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Prepare the frame\n",
    "    frame_resized = cv2.resize(frame, (input_size[0], input_size[1]))\n",
    "    blob = cv2.dnn.blobFromImage(frame_resized, scalefactor=1.0, size=(input_size[0], input_size[1]), mean=(104.0, 177.0, 123.0))\n",
    "\n",
    "    # Set the input to the network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Run the detection\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Ensure detections array is not empty and has expected dimensions\n",
    "    with open('emotion_results.txt', 'w') as file:\n",
    "      file.write(\"Detection results:\\n\")\n",
    "      file.write(\"Format: Emotion, Confidence, Box Coordinates (startX, startY, endX, endY)\\n\")\n",
    "      if detections is not None and len(detections.shape) == 4 and detections.shape[2] > 0:\n",
    "          # Loop over the detections\n",
    "          for i in range(0, detections.shape[2]):\n",
    "              confidence = detections[0, 0, i, 2]\n",
    "              if confidence > 0.5:\n",
    "                  # Compute the (x, y)-coordinates of the bounding box for the object\n",
    "                  box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "                  (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                  # Extract the ROI of the face\n",
    "                  face = frame[startY:endY, startX:endX]\n",
    "                  face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "                  face_resized = cv2.resize(face_gray, (64, 64)).reshape(1, 1, 64, 64)\n",
    "\n",
    "                  # Make a prediction on the ROI\n",
    "                  model.setInput(face_resized)\n",
    "                  preds = model.forward()\n",
    "                  emotion = emotion_dict[np.argmax(preds)]\n",
    "\n",
    "\n",
    "                  # Draw the face detection + emotion label on the image\n",
    "                  text = \"{}: {:.2f}%\".format(emotion, confidence * 100)\n",
    "                  cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                  cv2.putText(frame, text, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "                  file.write(f\"{emotion}, {confidence:.2f}, ({startX}, {startY}, {endX}, {endY})\\n\")\n",
    "\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite('result.jpg', frame)\n",
    "\n",
    "    # Display the image in the notebook\n",
    "    display(Image(filename='result.jpg'))\n",
    "\n",
    "\n",
    "# Capture an image from the webcam\n",
    "image_path = take_photo()\n",
    "\n",
    "# Process the captured image\n",
    "process_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-GdwY4kPBy5"
   },
   "source": [
    "# Another Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnxyD40XcQR_",
    "outputId": "4eda4f17-b4dc-4a90-f0d6-952920b65f69"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Securely input the API token\n",
    "sid = getpass.getpass(\"Enter your Twilio SID:\")\n",
    "os.environ[\"TWILIO_ACCOUNT_SID\"] = sid\n",
    "auth = getpass.getpass(\"Enter your Twilio auth:\")\n",
    "os.environ[\"TWILIO_AUTH_TOKEN\"] = auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kKMh_yfSpSs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import streamlit as st\n",
    "from twilio.base.exceptions import TwilioRestException\n",
    "from twilio.rest import Client\n",
    "\n",
    "\n",
    "def get_ice_servers():\n",
    "    \"\"\"Use Twilio's TURN server because Streamlit Community Cloud has changed\n",
    "    its infrastructure and WebRTC connection cannot be established without TURN server now.  # noqa: E501\n",
    "    We considered Open Relay Project (https://www.metered.ca/tools/openrelay/) too,\n",
    "    but it is not stable and hardly works as some people reported like https://github.com/aiortc/aiortc/issues/832#issuecomment-1482420656  # noqa: E501\n",
    "    See https://github.com/whitphx/streamlit-webrtc/issues/1213\n",
    "    \"\"\"\n",
    "\n",
    "    # Ref: https://www.twilio.com/docs/stun-turn/api\n",
    "    try:\n",
    "        account_sid = os.environ[\"TWILIO_ACCOUNT_SID\"]\n",
    "        auth_token = os.environ[\"TWILIO_AUTH_TOKEN\"]\n",
    "    except KeyError:\n",
    "        print(\n",
    "            \"Twilio credentials are not set. Fallback to a free STUN server from Google.\"  # noqa: E501\n",
    "        )\n",
    "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    try:\n",
    "        token = client.tokens.create()\n",
    "    except TwilioRestException as e:\n",
    "        st.warning(\n",
    "            f\"Error occurred while accessing Twilio API. Fallback to a free STUN server from Google. ({e})\"  # noqa: E501\n",
    "        )\n",
    "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
    "\n",
    "    return token.ice_servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QsPaDlrPBO0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Emotion Detection:\n",
    "Model from: https://github.com/onnx/models/blob/main/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx\n",
    "Model name: emotion-ferplus-8.onnx\n",
    "\"\"\"\n",
    "\n",
    "import queue\n",
    "from pathlib import Path\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from cv2 import dnn\n",
    "from math import ceil\n",
    "\n",
    "import av\n",
    "import streamlit as st\n",
    "\n",
    "HERE = Path(Path.cwd()).parent\n",
    "ROOT = HERE.parent\n",
    "\n",
    "\n",
    "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
    "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
    "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
    "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
    "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
    "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt.txt\"\n",
    "\n",
    "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
    "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
    "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH) #, expected_size=29353)\n",
    "\n",
    "# Session-specific caching\n",
    "onnx_cache_key = \"emotion_dnn\"\n",
    "caffe_cache_key = \"face_detection_dnn\"\n",
    "if caffe_cache_key in st.session_state and onnx_cache_key in st.session_state:\n",
    "  model = st.session_state[onnx_cache_key]\n",
    "  net = st.session_state[caffe_cache_key]\n",
    "else:\n",
    "  # Read ONNX model\n",
    "  model = 'onnx_model.onnx'\n",
    "  model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "  st.session_state[onnx_cache_key] = model\n",
    "  # Read the Caffe face detector.\n",
    "  net = cv2.dnn.readNetFromCaffe(str(PROTOTXT_LOCAL_PATH), str(CAFFE_MODEL_LOCAL_PATH))\n",
    "  st.session_state[caffe_cache_key] = net\n",
    "\n",
    "\n",
    "########################################\n",
    "image_mean = np.array([127, 127, 127])\n",
    "image_std = 128.0\n",
    "iou_threshold = 0.3\n",
    "center_variance = 0.1\n",
    "size_variance = 0.2\n",
    "min_boxes = [\n",
    "  [10.0, 16.0, 24.0],\n",
    "  [32.0, 48.0],\n",
    "  [64.0, 96.0],\n",
    "  [128.0, 192.0, 256.0]\n",
    "]\n",
    "strides = [8.0, 16.0, 32.0, 64.0]\n",
    "threshold = 0.5\n",
    "\n",
    "emotion_dict = {\n",
    "      0: 'neutral',\n",
    "      1: 'happiness',\n",
    "      2: 'surprise',\n",
    "      3: 'sadness',\n",
    "      4: 'anger',\n",
    "      5: 'disgust',\n",
    "      6: 'fear'\n",
    "  }\n",
    "########################################\n",
    "\n",
    "def define_img_size(image_size):\n",
    "  shrinkage_list = []\n",
    "  feature_map_w_h_list = []\n",
    "  for size in image_size:\n",
    "      feature_map = [int(ceil(size / stride)) for stride in strides]\n",
    "      feature_map_w_h_list.append(feature_map)\n",
    "\n",
    "  for i in range(0, len(image_size)):\n",
    "      shrinkage_list.append(strides)\n",
    "  priors = generate_priors(\n",
    "      feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
    "  )\n",
    "  return priors\n",
    "\n",
    "\n",
    "def generate_priors(\n",
    "  feature_map_list, shrinkage_list, image_size, min_boxes\n",
    "):\n",
    "  priors = []\n",
    "  for index in range(0, len(feature_map_list[0])):\n",
    "      scale_w = image_size[0] / shrinkage_list[0][index]\n",
    "      scale_h = image_size[1] / shrinkage_list[1][index]\n",
    "      for j in range(0, feature_map_list[1][index]):\n",
    "          for i in range(0, feature_map_list[0][index]):\n",
    "              x_center = (i + 0.5) / scale_w\n",
    "              y_center = (j + 0.5) / scale_h\n",
    "\n",
    "              for min_box in min_boxes[index]:\n",
    "                  w = min_box / image_size[0]\n",
    "                  h = min_box / image_size[1]\n",
    "                  priors.append([\n",
    "                      x_center,\n",
    "                      y_center,\n",
    "                      w,\n",
    "                      h\n",
    "                  ])\n",
    "  print(\"priors nums:{}\".format(len(priors)))\n",
    "  return np.clip(priors, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "  scores = box_scores[:, -1]\n",
    "  boxes = box_scores[:, :-1]\n",
    "  picked = []\n",
    "  indexes = np.argsort(scores)\n",
    "  indexes = indexes[-candidate_size:]\n",
    "  while len(indexes) > 0:\n",
    "      current = indexes[-1]\n",
    "      picked.append(current)\n",
    "      if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "          break\n",
    "      current_box = boxes[current, :]\n",
    "      indexes = indexes[:-1]\n",
    "      rest_boxes = boxes[indexes, :]\n",
    "      iou = iou_of(\n",
    "          rest_boxes,\n",
    "          np.expand_dims(current_box, axis=0),\n",
    "      )\n",
    "      indexes = indexes[iou <= iou_threshold]\n",
    "  return box_scores[picked, :]\n",
    "\n",
    "\n",
    "def area_of(left_top, right_bottom):\n",
    "  hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "  return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "  overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "  overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "  overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "  area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "  area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "  return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "\n",
    "def predict(\n",
    "  width,\n",
    "  height,\n",
    "  confidences,\n",
    "  boxes,\n",
    "  prob_threshold,\n",
    "  iou_threshold=0.3,\n",
    "  top_k=-1\n",
    "):\n",
    "  boxes = boxes[0]\n",
    "  confidences = confidences[0]\n",
    "  picked_box_probs = []\n",
    "  picked_labels = []\n",
    "  for class_index in range(1, confidences.shape[1]):\n",
    "      probs = confidences[:, class_index]\n",
    "      mask = probs > prob_threshold\n",
    "      probs = probs[mask]\n",
    "      if probs.shape[0] == 0:\n",
    "          continue\n",
    "      subset_boxes = boxes[mask, :]\n",
    "      box_probs = np.concatenate(\n",
    "          [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
    "      )\n",
    "      box_probs = hard_nms(box_probs,\n",
    "                            iou_threshold=iou_threshold,\n",
    "                            top_k=top_k,\n",
    "                            )\n",
    "      picked_box_probs.append(box_probs)\n",
    "      picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "  if not picked_box_probs:\n",
    "      return np.array([]), np.array([]), np.array([])\n",
    "  picked_box_probs = np.concatenate(picked_box_probs)\n",
    "  picked_box_probs[:, 0] *= width\n",
    "  picked_box_probs[:, 1] *= height\n",
    "  picked_box_probs[:, 2] *= width\n",
    "  picked_box_probs[:, 3] *= height\n",
    "  return (\n",
    "      picked_box_probs[:, :4].astype(np.int32),\n",
    "      np.array(picked_labels),\n",
    "      picked_box_probs[:, 4]\n",
    "  )\n",
    "\n",
    "\n",
    "def convert_locations_to_boxes(locations, priors, center_variance,\n",
    "                              size_variance):\n",
    "  if len(priors.shape) + 1 == len(locations.shape):\n",
    "      priors = np.expand_dims(priors, 0)\n",
    "  return np.concatenate([\n",
    "      locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
    "      np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "  ], axis=len(locations.shape) - 1)\n",
    "\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "  return np.concatenate(\n",
    "      [locations[..., :2] - locations[..., 2:] / 2,\n",
    "        locations[..., :2] + locations[..., 2:] / 2],\n",
    "      len(locations.shape) - 1\n",
    "  )\n",
    "\n",
    "\n",
    "# def FER_live_cam():\n",
    "def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "  frame = frame.to_ndarray(format=\"bgr24\")\n",
    "\n",
    "  # cap = cv2.VideoCapture('video3.mp4')\n",
    "  # cap = cv2.VideoCapture(0)\n",
    "\n",
    "  # frame_width = int(cap.get(3))\n",
    "  # frame_height = int(cap.get(4))\n",
    "  # size = (frame_width, frame_height)\n",
    "  # result = cv2.VideoWriter('infer2-test.avi',\n",
    "  #                      cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "  #                      10, size)\n",
    "  # Read ONNX model\n",
    "  # model = 'onnx_model.onnx'\n",
    "  # model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "\n",
    "  # # Read the Caffe face detector.\n",
    "  # model_path = 'RFB-320/RFB-320.caffemodel'\n",
    "  # proto_path = 'RFB-320/RFB-320.prototxt'\n",
    "  # net = dnn.readNetFromCaffe(proto_path, model_path)\n",
    "\n",
    "  input_size = [320, 240]\n",
    "  width = input_size[0]\n",
    "  height = input_size[1]\n",
    "  priors = define_img_size(input_size)\n",
    "\n",
    "  # while cap.isOpened():\n",
    "  #     ret, frame = cap.read()\n",
    "  #     if ret:\n",
    "  img_ori = frame\n",
    "  #print(\"frame size: \", frame.shape)\n",
    "  rect = cv2.resize(img_ori, (width, height))\n",
    "  rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
    "  net.setInput(dnn.blobFromImage(rect, 1 / image_std, (width, height), 127))\n",
    "  start_time = time.time()\n",
    "  boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
    "  boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
    "  scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
    "  boxes = convert_locations_to_boxes(\n",
    "      boxes, priors, center_variance, size_variance\n",
    "  )\n",
    "  boxes = center_form_to_corner_form(boxes)\n",
    "  boxes, labels, probs = predict(\n",
    "      img_ori.shape[1],\n",
    "      img_ori.shape[0],\n",
    "      scores,\n",
    "      boxes,\n",
    "      threshold\n",
    "  )\n",
    "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  for (x1, y1, x2, y2) in boxes:\n",
    "      w = x2 - x1\n",
    "      h = y2 - y1\n",
    "      cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
    "      resize_frame = cv2.resize(\n",
    "          gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
    "      )\n",
    "      resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
    "      model.setInput(resize_frame)\n",
    "      output = model.forward()\n",
    "      end_time = time.time()\n",
    "      fps = 1 / (end_time - start_time)\n",
    "      print(f\"FPS: {fps:.1f}\")\n",
    "      pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
    "      cv2.rectangle(\n",
    "          img_ori,\n",
    "          (x1, y1),\n",
    "          (x2, y2),\n",
    "          (215, 5, 247),\n",
    "          2,\n",
    "          lineType=cv2.LINE_AA\n",
    "      )\n",
    "      cv2.putText(\n",
    "          frame,\n",
    "          pred,\n",
    "          (x1, y1-10),\n",
    "          cv2.FONT_HERSHEY_SIMPLEX,\n",
    "          0.8,\n",
    "          (215, 5, 247),\n",
    "          2,\n",
    "          lineType=cv2.LINE_AA\n",
    "      )\n",
    "\n",
    "  #         result.write(frame)\n",
    "\n",
    "  #         cv2.imshow('frame', frame)\n",
    "  #         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "  #             break\n",
    "  #     else:\n",
    "  #         break\n",
    "\n",
    "  # cap.release()\n",
    "  # result.release()\n",
    "  # cv2.destroyAllWindows()\n",
    "  return av.VideoFrame.from_ndarray(frame, format=\"bgr24\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGyNtwdQUtrk"
   },
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r90uL31GQVBf"
   },
   "source": [
    "## Write Streamlit Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGsbqFlTQKX9",
    "outputId": "867d4956-2565-40ac-9d26-74761b1c58c0"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "import streamlit as st\n",
    "from streamlit_webrtc import webrtc_streamer, WebRtcMode\n",
    "from twilio.base.exceptions import TwilioRestException\n",
    "from twilio.rest import Client\n",
    "import queue\n",
    "import urllib.request\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "from typing import List, NamedTuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from cv2 import dnn\n",
    "from math import ceil\n",
    "\n",
    "import av\n",
    "import streamlit as st\n",
    "\n",
    "HERE = Path(Path.cwd()).parent\n",
    "ROOT = HERE.parent\n",
    "\n",
    "# This code is based on https://github.com/streamlit/demo-self-driving/blob/230245391f2dda0cb464008195a470751c01770b/streamlit_app.py#L48  # noqa: E501\n",
    "# and I, Emma, ripped this from https://huggingface.co/spaces/sadimanna/face-emotion-recognition/\n",
    "\n",
    "def download_file(url, download_to: Path, expected_size=None, key=None):\n",
    "    # Don't download the file twice.\n",
    "    # (If possible, verify the download using the file length.)\n",
    "    if download_to.exists():\n",
    "        if expected_size:\n",
    "            if download_to.stat().st_size == expected_size:\n",
    "                return\n",
    "        else:\n",
    "            st.info(f\"{url} is already downloaded.\")\n",
    "            if not st.button(\"Download again?\", key=key):\n",
    "                return\n",
    "\n",
    "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # These are handles to two visual elements to animate.\n",
    "    weights_warning, progress_bar = None, None\n",
    "    try:\n",
    "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
    "        progress_bar = st.progress(0)\n",
    "        with open(download_to, \"wb\") as output_file:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                length = int(response.info()[\"Content-Length\"])\n",
    "                counter = 0.0\n",
    "                MEGABYTES = 2.0 ** 20.0\n",
    "                while True:\n",
    "                    data = response.read(8192)\n",
    "                    if not data:\n",
    "                        break\n",
    "                    counter += len(data)\n",
    "                    output_file.write(data)\n",
    "\n",
    "                    # We perform animation by overwriting the elements.\n",
    "                    weights_warning.warning(\n",
    "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
    "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
    "                    )\n",
    "                    progress_bar.progress(min(counter / length, 1.0))\n",
    "    # Finally, we remove these visual elements by calling .empty().\n",
    "    finally:\n",
    "        if weights_warning is not None:\n",
    "            weights_warning.empty()\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.empty()\n",
    "\n",
    "\n",
    "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
    "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
    "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
    "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
    "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
    "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt.txt\"\n",
    "\n",
    "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH, key='caffe_model_download') #, expected_size=23147564)\n",
    "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH, key='onnx_model_download') #, expected_size=23147564)\n",
    "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH,  key='prototxt_download') #, expected_size=29353)\n",
    "\n",
    "# Session-specific caching\n",
    "onnx_cache_key = \"emotion_dnn\"\n",
    "caffe_cache_key = \"face_detection_dnn\"\n",
    "if caffe_cache_key in st.session_state and onnx_cache_key in st.session_state:\n",
    "  model = st.session_state[onnx_cache_key]\n",
    "  net = st.session_state[caffe_cache_key]\n",
    "else:\n",
    "  # Read ONNX model\n",
    "  model = 'onnx_model.onnx'\n",
    "  model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "  st.session_state[onnx_cache_key] = model\n",
    "  # Read the Caffe face detector.\n",
    "  net = cv2.dnn.readNetFromCaffe(str(PROTOTXT_LOCAL_PATH), str(CAFFE_MODEL_LOCAL_PATH))\n",
    "  st.session_state[caffe_cache_key] = net\n",
    "\n",
    "\n",
    "########################################\n",
    "image_mean = np.array([127, 127, 127])\n",
    "image_std = 128.0\n",
    "iou_threshold = 0.3\n",
    "center_variance = 0.1\n",
    "size_variance = 0.2\n",
    "min_boxes = [\n",
    "  [10.0, 16.0, 24.0],\n",
    "  [32.0, 48.0],\n",
    "  [64.0, 96.0],\n",
    "  [128.0, 192.0, 256.0]\n",
    "]\n",
    "strides = [8.0, 16.0, 32.0, 64.0]\n",
    "threshold = 0.5\n",
    "\n",
    "emotion_dict = {\n",
    "      0: 'neutral',\n",
    "      1: 'happiness',\n",
    "      2: 'surprise',\n",
    "      3: 'sadness',\n",
    "      4: 'anger',\n",
    "      5: 'disgust',\n",
    "      6: 'fear'\n",
    "  }\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "def define_img_size(image_size):\n",
    "  shrinkage_list = []\n",
    "  feature_map_w_h_list = []\n",
    "  for size in image_size:\n",
    "      feature_map = [int(ceil(size / stride)) for stride in strides]\n",
    "      feature_map_w_h_list.append(feature_map)\n",
    "\n",
    "  for i in range(0, len(image_size)):\n",
    "      shrinkage_list.append(strides)\n",
    "  priors = generate_priors(\n",
    "      feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
    "  )\n",
    "  return priors\n",
    "\n",
    "\n",
    "def generate_priors(\n",
    "  feature_map_list, shrinkage_list, image_size, min_boxes\n",
    "):\n",
    "  priors = []\n",
    "  for index in range(0, len(feature_map_list[0])):\n",
    "      scale_w = image_size[0] / shrinkage_list[0][index]\n",
    "      scale_h = image_size[1] / shrinkage_list[1][index]\n",
    "      for j in range(0, feature_map_list[1][index]):\n",
    "          for i in range(0, feature_map_list[0][index]):\n",
    "              x_center = (i + 0.5) / scale_w\n",
    "              y_center = (j + 0.5) / scale_h\n",
    "\n",
    "              for min_box in min_boxes[index]:\n",
    "                  w = min_box / image_size[0]\n",
    "                  h = min_box / image_size[1]\n",
    "                  priors.append([\n",
    "                      x_center,\n",
    "                      y_center,\n",
    "                      w,\n",
    "                      h\n",
    "                  ])\n",
    "  print(\"priors nums:{}\".format(len(priors)))\n",
    "  return np.clip(priors, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "  scores = box_scores[:, -1]\n",
    "  boxes = box_scores[:, :-1]\n",
    "  picked = []\n",
    "  indexes = np.argsort(scores)\n",
    "  indexes = indexes[-candidate_size:]\n",
    "  while len(indexes) > 0:\n",
    "      current = indexes[-1]\n",
    "      picked.append(current)\n",
    "      if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "          break\n",
    "      current_box = boxes[current, :]\n",
    "      indexes = indexes[:-1]\n",
    "      rest_boxes = boxes[indexes, :]\n",
    "      iou = iou_of(\n",
    "          rest_boxes,\n",
    "          np.expand_dims(current_box, axis=0),\n",
    "      )\n",
    "      indexes = indexes[iou <= iou_threshold]\n",
    "  return box_scores[picked, :]\n",
    "\n",
    "\n",
    "def area_of(left_top, right_bottom):\n",
    "  hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "  return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "  overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "  overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
    "\n",
    "  overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "  area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "  area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "  return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "\n",
    "def predict(\n",
    "  width,\n",
    "  height,\n",
    "  confidences,\n",
    "  boxes,\n",
    "  prob_threshold,\n",
    "  iou_threshold=0.3,\n",
    "  top_k=-1\n",
    "):\n",
    "  boxes = boxes[0]\n",
    "  confidences = confidences[0]\n",
    "  picked_box_probs = []\n",
    "  picked_labels = []\n",
    "  for class_index in range(1, confidences.shape[1]):\n",
    "      probs = confidences[:, class_index]\n",
    "      mask = probs > prob_threshold\n",
    "      probs = probs[mask]\n",
    "      if probs.shape[0] == 0:\n",
    "          continue\n",
    "      subset_boxes = boxes[mask, :]\n",
    "      box_probs = np.concatenate(\n",
    "          [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
    "      )\n",
    "      box_probs = hard_nms(box_probs,\n",
    "                            iou_threshold=iou_threshold,\n",
    "                            top_k=top_k,\n",
    "                            )\n",
    "      picked_box_probs.append(box_probs)\n",
    "      picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "  if not picked_box_probs:\n",
    "      return np.array([]), np.array([]), np.array([])\n",
    "  picked_box_probs = np.concatenate(picked_box_probs)\n",
    "  picked_box_probs[:, 0] *= width\n",
    "  picked_box_probs[:, 1] *= height\n",
    "  picked_box_probs[:, 2] *= width\n",
    "  picked_box_probs[:, 3] *= height\n",
    "  return (\n",
    "      picked_box_probs[:, :4].astype(np.int32),\n",
    "      np.array(picked_labels),\n",
    "      picked_box_probs[:, 4]\n",
    "  )\n",
    "\n",
    "\n",
    "def convert_locations_to_boxes(locations, priors, center_variance,\n",
    "                              size_variance):\n",
    "  if len(priors.shape) + 1 == len(locations.shape):\n",
    "      priors = np.expand_dims(priors, 0)\n",
    "  return np.concatenate([\n",
    "      locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
    "      np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "  ], axis=len(locations.shape) - 1)\n",
    "\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "  return np.concatenate(\n",
    "      [locations[..., :2] - locations[..., 2:] / 2,\n",
    "        locations[..., :2] + locations[..., 2:] / 2],\n",
    "      len(locations.shape) - 1\n",
    "  )\n",
    "\n",
    "\n",
    "# def FER_live_cam():\n",
    "def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
    "  frame = frame.to_ndarray(format=\"bgr24\")\n",
    "\n",
    "  # cap = cv2.VideoCapture('video3.mp4')\n",
    "  # cap = cv2.VideoCapture(0)\n",
    "\n",
    "  # frame_width = int(cap.get(3))\n",
    "  # frame_height = int(cap.get(4))\n",
    "  # size = (frame_width, frame_height)\n",
    "  # result = cv2.VideoWriter('infer2-test.avi',\n",
    "  #                      cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "  #                      10, size)\n",
    "  # Read ONNX model\n",
    "  # model = 'onnx_model.onnx'\n",
    "  # model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "\n",
    "  # # Read the Caffe face detector.\n",
    "  # model_path = 'RFB-320/RFB-320.caffemodel'\n",
    "  # proto_path = 'RFB-320/RFB-320.prototxt'\n",
    "  # net = dnn.readNetFromCaffe(proto_path, model_path)\n",
    "\n",
    "  input_size = [320, 240]\n",
    "  width = input_size[0]\n",
    "  height = input_size[1]\n",
    "  priors = define_img_size(input_size)\n",
    "\n",
    "  # while cap.isOpened():\n",
    "  #     ret, frame = cap.read()\n",
    "  #     if ret:\n",
    "  img_ori = frame\n",
    "  #print(\"frame size: \", frame.shape)\n",
    "  rect = cv2.resize(img_ori, (width, height))\n",
    "  rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
    "  net.setInput(dnn.blobFromImage(rect, 1 / image_std, (width, height), 127))\n",
    "  start_time = time.time()\n",
    "  boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
    "  boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
    "  scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
    "  boxes = convert_locations_to_boxes(\n",
    "      boxes, priors, center_variance, size_variance\n",
    "  )\n",
    "  boxes = center_form_to_corner_form(boxes)\n",
    "  boxes, labels, probs = predict(\n",
    "      img_ori.shape[1],\n",
    "      img_ori.shape[0],\n",
    "      scores,\n",
    "      boxes,\n",
    "      threshold\n",
    "  )\n",
    "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  for (x1, y1, x2, y2) in boxes:\n",
    "      w = x2 - x1\n",
    "      h = y2 - y1\n",
    "      cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
    "      resize_frame = cv2.resize(\n",
    "          gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
    "      )\n",
    "      resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
    "      model.setInput(resize_frame)\n",
    "      output = model.forward()\n",
    "      end_time = time.time()\n",
    "      fps = 1 / (end_time - start_time)\n",
    "      print(f\"FPS: {fps:.1f}\")\n",
    "      pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
    "      cv2.rectangle(\n",
    "          img_ori,\n",
    "          (x1, y1),\n",
    "          (x2, y2),\n",
    "          (215, 5, 247),\n",
    "          2,\n",
    "          lineType=cv2.LINE_AA\n",
    "      )\n",
    "      cv2.putText(\n",
    "          frame,\n",
    "          pred,\n",
    "          (x1, y1-10),\n",
    "          cv2.FONT_HERSHEY_SIMPLEX,\n",
    "          0.8,\n",
    "          (215, 5, 247),\n",
    "          2,\n",
    "          lineType=cv2.LINE_AA\n",
    "      )\n",
    "\n",
    "  #         result.write(frame)\n",
    "\n",
    "  #         cv2.imshow('frame', frame)\n",
    "  #         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "  #             break\n",
    "  #     else:\n",
    "  #         break\n",
    "\n",
    "  # cap.release()\n",
    "  # result.release()\n",
    "  # cv2.destroyAllWindows()\n",
    "  return av.VideoFrame.from_ndarray(frame, format=\"bgr24\")\n",
    "\n",
    "\n",
    "\n",
    "def get_ice_servers():\n",
    "    \"\"\"Use Twilio's TURN server because Streamlit Community Cloud has changed\n",
    "    its infrastructure and WebRTC connection cannot be established without TURN server now.  # noqa: E501\n",
    "    We considered Open Relay Project (https://www.metered.ca/tools/openrelay/) too,\n",
    "    but it is not stable and hardly works as some people reported like https://github.com/aiortc/aiortc/issues/832#issuecomment-1482420656  # noqa: E501\n",
    "    See https://github.com/whitphx/streamlit-webrtc/issues/1213\n",
    "    \"\"\"\n",
    "\n",
    "    # Ref: https://www.twilio.com/docs/stun-turn/api\n",
    "    try:\n",
    "        account_sid = os.environ[\"TWILIO_ACCOUNT_SID\"]\n",
    "        auth_token = os.environ[\"TWILIO_AUTH_TOKEN\"]\n",
    "    except KeyError:\n",
    "        print(\n",
    "            \"Twilio credentials are not set. Fallback to a free STUN server from Google.\"  # noqa: E501\n",
    "        )\n",
    "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
    "\n",
    "    client = Client(account_sid, auth_token)\n",
    "\n",
    "    try:\n",
    "        token = client.tokens.create()\n",
    "    except TwilioRestException as e:\n",
    "        st.warning(\n",
    "            f\"Error occurred while accessing Twilio API. Fallback to a free STUN server from Google. ({e})\"  # noqa: E501\n",
    "        )\n",
    "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
    "\n",
    "    return token.ice_servers\n",
    "\n",
    "def main():\n",
    "    st.title(\"WebRTC in Streamlit on Colab\")\n",
    "    ice_servers = get_ice_servers()\n",
    "    webrtc_ctx = webrtc_streamer(\n",
    "    key=\"object-detection\",\n",
    "    mode=WebRtcMode.SENDRECV,\n",
    "    rtc_configuration={\"iceServers\": ice_servers},\n",
    "    video_frame_callback=video_frame_callback,\n",
    "    media_stream_constraints={\"video\": True, \"audio\": False},\n",
    "    async_processing=True,\n",
    ")\n",
    "\n",
    "    if st.checkbox(\"Show the detected labels\", value=True):\n",
    "      if webrtc_ctx.state.playing:\n",
    "          labels_placeholder = st.empty()\n",
    "          # NOTE: The video transformation with object detection and\n",
    "          # this loop displaying the result labels are running\n",
    "          # in different threads asynchronously.\n",
    "          # Then the rendered video frames and the labels displayed here\n",
    "          # are not strictly synchronized.\n",
    "\n",
    "    st.markdown(\n",
    "    \"This demo uses a model and code from \"\n",
    "    \"https://github.com/spmallick/learnopncv/Facial-Emotion-Recognition. \"\n",
    "    \"Many thanks to the project.\"\n",
    "  )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaZ2RMaTQXZf"
   },
   "source": [
    "## Start ngrok Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_e5-KttQaAG",
    "outputId": "6b7b8bf5-5060-4da2-f3cb-63a04bbc4029"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Setup a tunnel to the streamlit port 8501\n",
    "ngrok.set_auth_token('2g4zHuxbtXn6eAFHzTCbyOt4ZH4_5B1myF1riDk7YkhU5mcea')\n",
    "public_url = ngrok.connect(addr='8501')\n",
    "public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILsRlsqXQcY8"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py &>/dev/null&\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKnqtv77TYOk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO/mZ5BaEiaEu6qn1kJGHXY",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
