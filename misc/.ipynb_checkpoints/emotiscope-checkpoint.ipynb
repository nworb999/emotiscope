{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nworb999/emotiscope/blob/main/emotiscope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg01WmrKT_j3"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cc5kXa335Vu",
        "outputId": "915024f3-f4ac-4a56-d38f-8e0412ad2bd2"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers scipy ftfy accelerate torch torchvision np opencv-python streamlit streamlit-webrtc av pyngrok twilio -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wu-C63erGuw"
      },
      "source": [
        "#Hugging Face Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Au_xHFa_19o",
        "outputId": "9af8a763-07e6-4a94-d91e-99528b7d138e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Securely input the API token\n",
        "api_token = getpass.getpass(\"Enter your API token:\")\n",
        "os.environ[\"HF_TOKEN\"] = api_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd0m4JU4Uyx-"
      },
      "source": [
        "# Clear CUDA cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Jj2olJUyPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hlA7jukuIv4"
      },
      "source": [
        "# Facial Expression Recognition Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsNmAw7_vmEP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import cv2 as cv2\n",
        "import time as time\n",
        "\n",
        "image_mean = np.array([127, 127, 127])\n",
        "image_std = 128.0\n",
        "iou_threshold = 0.3\n",
        "center_variance = 0.1\n",
        "size_variance = 0.2\n",
        "min_boxes = [\n",
        "    [10.0, 16.0, 24.0],\n",
        "    [32.0, 48.0],\n",
        "    [64.0, 96.0],\n",
        "    [128.0, 192.0, 256.0]\n",
        "]\n",
        "strides = [8.0, 16.0, 32.0, 64.0]\n",
        "threshold = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5kTIbUayQTy"
      },
      "outputs": [],
      "source": [
        "def generate_priors(feature_map_list, shrinkage_list, image_size, min_boxes):\n",
        "    priors = []\n",
        "    for index in range(0, len(feature_map_list[0])):\n",
        "        scale_w = image_size[0] / shrinkage_list[0][index]\n",
        "        scale_h = image_size[1] / shrinkage_list[1][index]\n",
        "        for j in range(0, feature_map_list[1][index]):\n",
        "            for i in range(0, feature_map_list[0][index]):\n",
        "                x_center = (i + 0.5) / scale_w\n",
        "                y_center = (j + 0.5) / scale_h\n",
        "\n",
        "                for min_box in min_boxes[index]:\n",
        "                    w = min_box / image_size[0]\n",
        "                    h = min_box / image_size[1]\n",
        "                    priors.append([\n",
        "                        x_center,\n",
        "                        y_center,\n",
        "                        w,\n",
        "                        h\n",
        "                    ])\n",
        "    print(\"priors nums:{}\".format(len(priors)))\n",
        "    return np.clip(priors, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxT-PFnbxQKZ"
      },
      "outputs": [],
      "source": [
        "def define_img_size(image_size):\n",
        "    shrinkage_list = []\n",
        "    feature_map_w_h_list = []\n",
        "    for size in image_size:\n",
        "        feature_map = [int(math.ceil(size / stride)) for stride in strides]\n",
        "        feature_map_w_h_list.append(feature_map)\n",
        "\n",
        "    for i in range(0, len(image_size)):\n",
        "        shrinkage_list.append(strides)\n",
        "    priors = generate_priors(\n",
        "        feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
        "    )\n",
        "    return priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0f9Bb3vzG9O"
      },
      "outputs": [],
      "source": [
        "def area_of(left_top, right_bottom):\n",
        "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
        "    return hw[..., 0] * hw[..., 1]\n",
        "\n",
        "\n",
        "def iou_of(boxes0, boxes1, eps=1e-5):\n",
        "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
        "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
        "\n",
        "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
        "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
        "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
        "    return overlap_area / (area0 + area1 - overlap_area + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsb0kCglzCSw"
      },
      "outputs": [],
      "source": [
        "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
        "    scores = box_scores[:, -1]\n",
        "    boxes = box_scores[:, :-1]\n",
        "    picked = []\n",
        "    indexes = np.argsort(scores)\n",
        "    indexes = indexes[-candidate_size:]\n",
        "    while len(indexes) > 0:\n",
        "        current = indexes[-1]\n",
        "        picked.append(current)\n",
        "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
        "            break\n",
        "        current_box = boxes[current, :]\n",
        "        indexes = indexes[:-1]\n",
        "        rest_boxes = boxes[indexes, :]\n",
        "        iou = iou_of(\n",
        "            rest_boxes,\n",
        "            np.expand_dims(current_box, axis=0),\n",
        "        )\n",
        "        indexes = indexes[iou <= iou_threshold]\n",
        "    return box_scores[picked, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoz9WhagzIlr"
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    width,\n",
        "    height,\n",
        "    confidences,\n",
        "    boxes,\n",
        "    prob_threshold,\n",
        "    iou_threshold=0.3,\n",
        "    top_k=-1\n",
        "):\n",
        "    boxes = boxes[0]\n",
        "    confidences = confidences[0]\n",
        "    picked_box_probs = []\n",
        "    picked_labels = []\n",
        "    for class_index in range(1, confidences.shape[1]):\n",
        "        probs = confidences[:, class_index]\n",
        "        mask = probs > prob_threshold\n",
        "        probs = probs[mask]\n",
        "        if probs.shape[0] == 0:\n",
        "            continue\n",
        "        subset_boxes = boxes[mask, :]\n",
        "        box_probs = np.concatenate(\n",
        "            [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
        "        )\n",
        "        box_probs = hard_nms(box_probs,\n",
        "                             iou_threshold=iou_threshold,\n",
        "                             top_k=top_k,\n",
        "                             )\n",
        "        picked_box_probs.append(box_probs)\n",
        "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
        "    if not picked_box_probs:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    picked_box_probs = np.concatenate(picked_box_probs)\n",
        "    picked_box_probs[:, 0] *= width\n",
        "    picked_box_probs[:, 1] *= height\n",
        "    picked_box_probs[:, 2] *= width\n",
        "    picked_box_probs[:, 3] *= height\n",
        "    return (\n",
        "        picked_box_probs[:, :4].astype(np.int32),\n",
        "        np.array(picked_labels),\n",
        "        picked_box_probs[:, 4]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCCHOR7cz_w6"
      },
      "outputs": [],
      "source": [
        "def convert_locations_to_boxes(locations, priors, center_variance,\n",
        "                               size_variance):\n",
        "    if len(priors.shape) + 1 == len(locations.shape):\n",
        "        priors = np.expand_dims(priors, 0)\n",
        "    return np.concatenate([\n",
        "        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
        "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
        "    ], axis=len(locations.shape) - 1)\n",
        "\n",
        "\n",
        "def center_form_to_corner_form(locations):\n",
        "    return np.concatenate(\n",
        "        [locations[..., :2] - locations[..., 2:] / 2,\n",
        "         locations[..., :2] + locations[..., 2:] / 2],\n",
        "        len(locations.shape) - 1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7wkzsfP8sp2"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# This code is based on https://github.com/streamlit/demo-self-driving/blob/230245391f2dda0cb464008195a470751c01770b/streamlit_app.py#L48  # noqa: E501\n",
        "# and I, Emma, ripped this from https://huggingface.co/spaces/sadimanna/face-emotion-recognition/\n",
        "\n",
        "def download_file(url, download_to: Path, expected_size=None):\n",
        "    # Don't download the file twice.\n",
        "    # (If possible, verify the download using the file length.)\n",
        "    if download_to.exists():\n",
        "        if expected_size:\n",
        "            if download_to.stat().st_size == expected_size:\n",
        "                return\n",
        "        else:\n",
        "            st.info(f\"{url} is already downloaded.\")\n",
        "            if not st.button(\"Download again?\"):\n",
        "                return\n",
        "\n",
        "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # These are handles to two visual elements to animate.\n",
        "    weights_warning, progress_bar = None, None\n",
        "    try:\n",
        "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
        "        progress_bar = st.progress(0)\n",
        "        with open(download_to, \"wb\") as output_file:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                length = int(response.info()[\"Content-Length\"])\n",
        "                counter = 0.0\n",
        "                MEGABYTES = 2.0 ** 20.0\n",
        "                while True:\n",
        "                    data = response.read(8192)\n",
        "                    if not data:\n",
        "                        break\n",
        "                    counter += len(data)\n",
        "                    output_file.write(data)\n",
        "\n",
        "                    # We perform animation by overwriting the elements.\n",
        "                    weights_warning.warning(\n",
        "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
        "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
        "                    )\n",
        "                    progress_bar.progress(min(counter / length, 1.0))\n",
        "    # Finally, we remove these visual elements by calling .empty().\n",
        "    finally:\n",
        "        if weights_warning is not None:\n",
        "            weights_warning.empty()\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.empty()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifyIFtvm8ZF4"
      },
      "outputs": [],
      "source": [
        "HERE = Path.cwd()\n",
        "ROOT = HERE\n",
        "\n",
        "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
        "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
        "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
        "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
        "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
        "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt\"\n",
        "\n",
        "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH) #, expected_size=29353)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MB03ztJZXp"
      },
      "source": [
        "# Live Camera Capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqFBfGfu0iYI"
      },
      "outputs": [],
      "source": [
        "def FER_live_cam():\n",
        "    emotion_dict = {\n",
        "        0: 'neutral',\n",
        "        1: 'happiness',\n",
        "        2: 'surprise',\n",
        "        3: 'sadness',\n",
        "        4: 'anger',\n",
        "        5: 'disgust',\n",
        "        6: 'fear'\n",
        "    }\n",
        "\n",
        "    # cap = cv2.VideoCapture('video1.mp4')\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    size = (frame_width, frame_height)\n",
        "    result = cv2.VideoWriter('result.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         10, size)\n",
        "\n",
        "    # Read ONNX model\n",
        "    # model = 'onnx_model.onnx'\n",
        "    model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "\n",
        "    # Read the Caffe face detector.\n",
        "    model_path = 'RFB-320/RFB-320.caffemodel'\n",
        "    proto_path = 'RFB-320/RFB-320.prototxt'\n",
        "    net = cv2.dnn.readNetFromCaffe(proto_path, model_path)\n",
        "    input_size = [320, 240]\n",
        "    width = input_size[0]\n",
        "    height = input_size[1]\n",
        "    priors = define_img_size(input_size)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            img_ori = frame\n",
        "            #print(\"frame size: \", frame.shape)\n",
        "            rect = cv2.resize(img_ori, (width, height))\n",
        "            rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
        "            net.setInput(cv2.dnn.blobFromImage(\n",
        "                rect, 1 / image_std, (width, height), 127)\n",
        "            )\n",
        "            start_time = time.time()\n",
        "            boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
        "            boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
        "            scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
        "            boxes = convert_locations_to_boxes(\n",
        "                boxes, priors, center_variance, size_variance\n",
        "            )\n",
        "            boxes = center_form_to_corner_form(boxes)\n",
        "            boxes, labels, probs = predict(\n",
        "                img_ori.shape[1],\n",
        "                img_ori.shape[0],\n",
        "                scores,\n",
        "                boxes,\n",
        "                threshold\n",
        "            )\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            for (x1, y1, x2, y2) in boxes:\n",
        "                w = x2 - x1\n",
        "                h = y2 - y1\n",
        "                cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
        "                resize_frame = cv2.resize(\n",
        "                    gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
        "                )\n",
        "                resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
        "                model.setInput(resize_frame)\n",
        "                output = model.forward()\n",
        "                end_time = time.time()\n",
        "                fps = 1 / (end_time - start_time)\n",
        "                print(f\"FPS: {fps:.1f}\")\n",
        "                pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
        "                cv2.rectangle(\n",
        "                    img_ori,\n",
        "                    (x1, y1),\n",
        "                    (x2, y2),\n",
        "                    (0, 255, 0),\n",
        "                    2,\n",
        "                    lineType=cv2.LINE_AA\n",
        "                )\n",
        "                cv2.putText(\n",
        "                    frame,\n",
        "                    pred,\n",
        "                    (x1, y1),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8,\n",
        "                    (0, 255, 0),\n",
        "                    2,\n",
        "                    lineType=cv2.LINE_AA\n",
        "                )\n",
        "\n",
        "            result.write(frame)\n",
        "\n",
        "            cv2.imshow('frame', frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    result.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "obOoWp7q01et",
        "outputId": "5e72861a-bbe1-452b-e242-b4feeffa62b0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "# Define the emotion dictionary\n",
        "emotion_dict = {\n",
        "    0: 'neutral', 1: 'happiness', 2: 'surprise', 3: 'sadness',\n",
        "    4: 'anger', 5: 'disgust', 6: 'fear'\n",
        "}\n",
        "\n",
        "# Load models\n",
        "# Assuming you have already downloaded these models to your Colab environment\n",
        "model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "net = cv2.dnn.readNetFromCaffe('RFB-320/RFB-320.prototxt', 'RFB-320/RFB-320.caffemodel')\n",
        "\n",
        "# Define the image size for the face detector\n",
        "input_size = [320, 240]\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def process_image(image_path):\n",
        "    # Read the image\n",
        "    frame = cv2.imread(image_path)\n",
        "\n",
        "    # Prepare the frame\n",
        "    frame_resized = cv2.resize(frame, (input_size[0], input_size[1]))\n",
        "    blob = cv2.dnn.blobFromImage(frame_resized, scalefactor=1.0, size=(input_size[0], input_size[1]), mean=(104.0, 177.0, 123.0))\n",
        "\n",
        "    # Set the input to the network\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Run the detection\n",
        "    detections = net.forward()\n",
        "\n",
        "    # Ensure detections array is not empty and has expected dimensions\n",
        "    with open('emotion_results.txt', 'w') as file:\n",
        "      file.write(\"Detection results:\\n\")\n",
        "      file.write(\"Format: Emotion, Confidence, Box Coordinates (startX, startY, endX, endY)\\n\")\n",
        "      if detections is not None and len(detections.shape) == 4 and detections.shape[2] > 0:\n",
        "          # Loop over the detections\n",
        "          for i in range(0, detections.shape[2]):\n",
        "              confidence = detections[0, 0, i, 2]\n",
        "              if confidence > 0.5:\n",
        "                  # Compute the (x, y)-coordinates of the bounding box for the object\n",
        "                  box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
        "                  (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                  # Extract the ROI of the face\n",
        "                  face = frame[startY:endY, startX:endX]\n",
        "                  face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "                  face_resized = cv2.resize(face_gray, (64, 64)).reshape(1, 1, 64, 64)\n",
        "\n",
        "                  # Make a prediction on the ROI\n",
        "                  model.setInput(face_resized)\n",
        "                  preds = model.forward()\n",
        "                  emotion = emotion_dict[np.argmax(preds)]\n",
        "\n",
        "\n",
        "                  # Draw the face detection + emotion label on the image\n",
        "                  text = \"{}: {:.2f}%\".format(emotion, confidence * 100)\n",
        "                  cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
        "                  cv2.putText(frame, text, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
        "                  file.write(f\"{emotion}, {confidence:.2f}, ({startX}, {startY}, {endX}, {endY})\\n\")\n",
        "\n",
        "\n",
        "    # Save the result\n",
        "    cv2.imwrite('result.jpg', frame)\n",
        "\n",
        "    # Display the image in the notebook\n",
        "    display(Image(filename='result.jpg'))\n",
        "\n",
        "\n",
        "# Capture an image from the webcam\n",
        "image_path = take_photo()\n",
        "\n",
        "# Process the captured image\n",
        "process_image(image_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-GdwY4kPBy5"
      },
      "source": [
        "# Another Try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnxyD40XcQR_",
        "outputId": "4eda4f17-b4dc-4a90-f0d6-952920b65f69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Securely input the API token\n",
        "sid = getpass.getpass(\"Enter your Twilio SID:\")\n",
        "os.environ[\"TWILIO_ACCOUNT_SID\"] = sid\n",
        "auth = getpass.getpass(\"Enter your Twilio auth:\")\n",
        "os.environ[\"TWILIO_AUTH_TOKEN\"] = auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kKMh_yfSpSs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import streamlit as st\n",
        "from twilio.base.exceptions import TwilioRestException\n",
        "from twilio.rest import Client\n",
        "\n",
        "\n",
        "def get_ice_servers():\n",
        "    \"\"\"Use Twilio's TURN server because Streamlit Community Cloud has changed\n",
        "    its infrastructure and WebRTC connection cannot be established without TURN server now.  # noqa: E501\n",
        "    We considered Open Relay Project (https://www.metered.ca/tools/openrelay/) too,\n",
        "    but it is not stable and hardly works as some people reported like https://github.com/aiortc/aiortc/issues/832#issuecomment-1482420656  # noqa: E501\n",
        "    See https://github.com/whitphx/streamlit-webrtc/issues/1213\n",
        "    \"\"\"\n",
        "\n",
        "    # Ref: https://www.twilio.com/docs/stun-turn/api\n",
        "    try:\n",
        "        account_sid = os.environ[\"TWILIO_ACCOUNT_SID\"]\n",
        "        auth_token = os.environ[\"TWILIO_AUTH_TOKEN\"]\n",
        "    except KeyError:\n",
        "        print(\n",
        "            \"Twilio credentials are not set. Fallback to a free STUN server from Google.\"  # noqa: E501\n",
        "        )\n",
        "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
        "\n",
        "    client = Client(account_sid, auth_token)\n",
        "\n",
        "    try:\n",
        "        token = client.tokens.create()\n",
        "    except TwilioRestException as e:\n",
        "        st.warning(\n",
        "            f\"Error occurred while accessing Twilio API. Fallback to a free STUN server from Google. ({e})\"  # noqa: E501\n",
        "        )\n",
        "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
        "\n",
        "    return token.ice_servers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QsPaDlrPBO0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Emotion Detection:\n",
        "Model from: https://github.com/onnx/models/blob/main/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx\n",
        "Model name: emotion-ferplus-8.onnx\n",
        "\"\"\"\n",
        "\n",
        "import queue\n",
        "from pathlib import Path\n",
        "from typing import List, NamedTuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from cv2 import dnn\n",
        "from math import ceil\n",
        "\n",
        "import av\n",
        "import streamlit as st\n",
        "\n",
        "HERE = Path(Path.cwd()).parent\n",
        "ROOT = HERE.parent\n",
        "\n",
        "\n",
        "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
        "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
        "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
        "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
        "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
        "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt.txt\"\n",
        "\n",
        "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH) #, expected_size=29353)\n",
        "\n",
        "# Session-specific caching\n",
        "onnx_cache_key = \"emotion_dnn\"\n",
        "caffe_cache_key = \"face_detection_dnn\"\n",
        "if caffe_cache_key in st.session_state and onnx_cache_key in st.session_state:\n",
        "  model = st.session_state[onnx_cache_key]\n",
        "  net = st.session_state[caffe_cache_key]\n",
        "else:\n",
        "  # Read ONNX model\n",
        "  model = 'onnx_model.onnx'\n",
        "  model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "  st.session_state[onnx_cache_key] = model\n",
        "  # Read the Caffe face detector.\n",
        "  net = cv2.dnn.readNetFromCaffe(str(PROTOTXT_LOCAL_PATH), str(CAFFE_MODEL_LOCAL_PATH))\n",
        "  st.session_state[caffe_cache_key] = net\n",
        "\n",
        "\n",
        "########################################\n",
        "image_mean = np.array([127, 127, 127])\n",
        "image_std = 128.0\n",
        "iou_threshold = 0.3\n",
        "center_variance = 0.1\n",
        "size_variance = 0.2\n",
        "min_boxes = [\n",
        "  [10.0, 16.0, 24.0],\n",
        "  [32.0, 48.0],\n",
        "  [64.0, 96.0],\n",
        "  [128.0, 192.0, 256.0]\n",
        "]\n",
        "strides = [8.0, 16.0, 32.0, 64.0]\n",
        "threshold = 0.5\n",
        "\n",
        "emotion_dict = {\n",
        "      0: 'neutral',\n",
        "      1: 'happiness',\n",
        "      2: 'surprise',\n",
        "      3: 'sadness',\n",
        "      4: 'anger',\n",
        "      5: 'disgust',\n",
        "      6: 'fear'\n",
        "  }\n",
        "########################################\n",
        "\n",
        "def define_img_size(image_size):\n",
        "  shrinkage_list = []\n",
        "  feature_map_w_h_list = []\n",
        "  for size in image_size:\n",
        "      feature_map = [int(ceil(size / stride)) for stride in strides]\n",
        "      feature_map_w_h_list.append(feature_map)\n",
        "\n",
        "  for i in range(0, len(image_size)):\n",
        "      shrinkage_list.append(strides)\n",
        "  priors = generate_priors(\n",
        "      feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
        "  )\n",
        "  return priors\n",
        "\n",
        "\n",
        "def generate_priors(\n",
        "  feature_map_list, shrinkage_list, image_size, min_boxes\n",
        "):\n",
        "  priors = []\n",
        "  for index in range(0, len(feature_map_list[0])):\n",
        "      scale_w = image_size[0] / shrinkage_list[0][index]\n",
        "      scale_h = image_size[1] / shrinkage_list[1][index]\n",
        "      for j in range(0, feature_map_list[1][index]):\n",
        "          for i in range(0, feature_map_list[0][index]):\n",
        "              x_center = (i + 0.5) / scale_w\n",
        "              y_center = (j + 0.5) / scale_h\n",
        "\n",
        "              for min_box in min_boxes[index]:\n",
        "                  w = min_box / image_size[0]\n",
        "                  h = min_box / image_size[1]\n",
        "                  priors.append([\n",
        "                      x_center,\n",
        "                      y_center,\n",
        "                      w,\n",
        "                      h\n",
        "                  ])\n",
        "  print(\"priors nums:{}\".format(len(priors)))\n",
        "  return np.clip(priors, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
        "  scores = box_scores[:, -1]\n",
        "  boxes = box_scores[:, :-1]\n",
        "  picked = []\n",
        "  indexes = np.argsort(scores)\n",
        "  indexes = indexes[-candidate_size:]\n",
        "  while len(indexes) > 0:\n",
        "      current = indexes[-1]\n",
        "      picked.append(current)\n",
        "      if 0 < top_k == len(picked) or len(indexes) == 1:\n",
        "          break\n",
        "      current_box = boxes[current, :]\n",
        "      indexes = indexes[:-1]\n",
        "      rest_boxes = boxes[indexes, :]\n",
        "      iou = iou_of(\n",
        "          rest_boxes,\n",
        "          np.expand_dims(current_box, axis=0),\n",
        "      )\n",
        "      indexes = indexes[iou <= iou_threshold]\n",
        "  return box_scores[picked, :]\n",
        "\n",
        "\n",
        "def area_of(left_top, right_bottom):\n",
        "  hw = np.clip(right_bottom - left_top, 0.0, None)\n",
        "  return hw[..., 0] * hw[..., 1]\n",
        "\n",
        "\n",
        "def iou_of(boxes0, boxes1, eps=1e-5):\n",
        "  overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
        "  overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
        "\n",
        "  overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
        "  area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
        "  area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
        "  return overlap_area / (area0 + area1 - overlap_area + eps)\n",
        "\n",
        "\n",
        "def predict(\n",
        "  width,\n",
        "  height,\n",
        "  confidences,\n",
        "  boxes,\n",
        "  prob_threshold,\n",
        "  iou_threshold=0.3,\n",
        "  top_k=-1\n",
        "):\n",
        "  boxes = boxes[0]\n",
        "  confidences = confidences[0]\n",
        "  picked_box_probs = []\n",
        "  picked_labels = []\n",
        "  for class_index in range(1, confidences.shape[1]):\n",
        "      probs = confidences[:, class_index]\n",
        "      mask = probs > prob_threshold\n",
        "      probs = probs[mask]\n",
        "      if probs.shape[0] == 0:\n",
        "          continue\n",
        "      subset_boxes = boxes[mask, :]\n",
        "      box_probs = np.concatenate(\n",
        "          [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
        "      )\n",
        "      box_probs = hard_nms(box_probs,\n",
        "                            iou_threshold=iou_threshold,\n",
        "                            top_k=top_k,\n",
        "                            )\n",
        "      picked_box_probs.append(box_probs)\n",
        "      picked_labels.extend([class_index] * box_probs.shape[0])\n",
        "  if not picked_box_probs:\n",
        "      return np.array([]), np.array([]), np.array([])\n",
        "  picked_box_probs = np.concatenate(picked_box_probs)\n",
        "  picked_box_probs[:, 0] *= width\n",
        "  picked_box_probs[:, 1] *= height\n",
        "  picked_box_probs[:, 2] *= width\n",
        "  picked_box_probs[:, 3] *= height\n",
        "  return (\n",
        "      picked_box_probs[:, :4].astype(np.int32),\n",
        "      np.array(picked_labels),\n",
        "      picked_box_probs[:, 4]\n",
        "  )\n",
        "\n",
        "\n",
        "def convert_locations_to_boxes(locations, priors, center_variance,\n",
        "                              size_variance):\n",
        "  if len(priors.shape) + 1 == len(locations.shape):\n",
        "      priors = np.expand_dims(priors, 0)\n",
        "  return np.concatenate([\n",
        "      locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
        "      np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
        "  ], axis=len(locations.shape) - 1)\n",
        "\n",
        "\n",
        "def center_form_to_corner_form(locations):\n",
        "  return np.concatenate(\n",
        "      [locations[..., :2] - locations[..., 2:] / 2,\n",
        "        locations[..., :2] + locations[..., 2:] / 2],\n",
        "      len(locations.shape) - 1\n",
        "  )\n",
        "\n",
        "\n",
        "# def FER_live_cam():\n",
        "def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
        "  frame = frame.to_ndarray(format=\"bgr24\")\n",
        "\n",
        "  # cap = cv2.VideoCapture('video3.mp4')\n",
        "  # cap = cv2.VideoCapture(0)\n",
        "\n",
        "  # frame_width = int(cap.get(3))\n",
        "  # frame_height = int(cap.get(4))\n",
        "  # size = (frame_width, frame_height)\n",
        "  # result = cv2.VideoWriter('infer2-test.avi',\n",
        "  #                      cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "  #                      10, size)\n",
        "  # Read ONNX model\n",
        "  # model = 'onnx_model.onnx'\n",
        "  # model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "\n",
        "  # # Read the Caffe face detector.\n",
        "  # model_path = 'RFB-320/RFB-320.caffemodel'\n",
        "  # proto_path = 'RFB-320/RFB-320.prototxt'\n",
        "  # net = dnn.readNetFromCaffe(proto_path, model_path)\n",
        "\n",
        "  input_size = [320, 240]\n",
        "  width = input_size[0]\n",
        "  height = input_size[1]\n",
        "  priors = define_img_size(input_size)\n",
        "\n",
        "  # while cap.isOpened():\n",
        "  #     ret, frame = cap.read()\n",
        "  #     if ret:\n",
        "  img_ori = frame\n",
        "  #print(\"frame size: \", frame.shape)\n",
        "  rect = cv2.resize(img_ori, (width, height))\n",
        "  rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
        "  net.setInput(dnn.blobFromImage(rect, 1 / image_std, (width, height), 127))\n",
        "  start_time = time.time()\n",
        "  boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
        "  boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
        "  scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
        "  boxes = convert_locations_to_boxes(\n",
        "      boxes, priors, center_variance, size_variance\n",
        "  )\n",
        "  boxes = center_form_to_corner_form(boxes)\n",
        "  boxes, labels, probs = predict(\n",
        "      img_ori.shape[1],\n",
        "      img_ori.shape[0],\n",
        "      scores,\n",
        "      boxes,\n",
        "      threshold\n",
        "  )\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  for (x1, y1, x2, y2) in boxes:\n",
        "      w = x2 - x1\n",
        "      h = y2 - y1\n",
        "      cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
        "      resize_frame = cv2.resize(\n",
        "          gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
        "      )\n",
        "      resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
        "      model.setInput(resize_frame)\n",
        "      output = model.forward()\n",
        "      end_time = time.time()\n",
        "      fps = 1 / (end_time - start_time)\n",
        "      print(f\"FPS: {fps:.1f}\")\n",
        "      pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
        "      cv2.rectangle(\n",
        "          img_ori,\n",
        "          (x1, y1),\n",
        "          (x2, y2),\n",
        "          (215, 5, 247),\n",
        "          2,\n",
        "          lineType=cv2.LINE_AA\n",
        "      )\n",
        "      cv2.putText(\n",
        "          frame,\n",
        "          pred,\n",
        "          (x1, y1-10),\n",
        "          cv2.FONT_HERSHEY_SIMPLEX,\n",
        "          0.8,\n",
        "          (215, 5, 247),\n",
        "          2,\n",
        "          lineType=cv2.LINE_AA\n",
        "      )\n",
        "\n",
        "  #         result.write(frame)\n",
        "\n",
        "  #         cv2.imshow('frame', frame)\n",
        "  #         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "  #             break\n",
        "  #     else:\n",
        "  #         break\n",
        "\n",
        "  # cap.release()\n",
        "  # result.release()\n",
        "  # cv2.destroyAllWindows()\n",
        "  return av.VideoFrame.from_ndarray(frame, format=\"bgr24\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGyNtwdQUtrk"
      },
      "source": [
        "# Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r90uL31GQVBf"
      },
      "source": [
        "## Write Streamlit Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGsbqFlTQKX9",
        "outputId": "867d4956-2565-40ac-9d26-74761b1c58c0"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from streamlit_webrtc import webrtc_streamer, WebRtcMode\n",
        "from twilio.base.exceptions import TwilioRestException\n",
        "from twilio.rest import Client\n",
        "import queue\n",
        "import urllib.request\n",
        "import streamlit as st\n",
        "from pathlib import Path\n",
        "from typing import List, NamedTuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from cv2 import dnn\n",
        "from math import ceil\n",
        "\n",
        "import av\n",
        "import streamlit as st\n",
        "\n",
        "HERE = Path(Path.cwd()).parent\n",
        "ROOT = HERE.parent\n",
        "\n",
        "# This code is based on https://github.com/streamlit/demo-self-driving/blob/230245391f2dda0cb464008195a470751c01770b/streamlit_app.py#L48  # noqa: E501\n",
        "# and I, Emma, ripped this from https://huggingface.co/spaces/sadimanna/face-emotion-recognition/\n",
        "\n",
        "def download_file(url, download_to: Path, expected_size=None, key=None):\n",
        "    # Don't download the file twice.\n",
        "    # (If possible, verify the download using the file length.)\n",
        "    if download_to.exists():\n",
        "        if expected_size:\n",
        "            if download_to.stat().st_size == expected_size:\n",
        "                return\n",
        "        else:\n",
        "            st.info(f\"{url} is already downloaded.\")\n",
        "            if not st.button(\"Download again?\", key=key):\n",
        "                return\n",
        "\n",
        "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # These are handles to two visual elements to animate.\n",
        "    weights_warning, progress_bar = None, None\n",
        "    try:\n",
        "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
        "        progress_bar = st.progress(0)\n",
        "        with open(download_to, \"wb\") as output_file:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                length = int(response.info()[\"Content-Length\"])\n",
        "                counter = 0.0\n",
        "                MEGABYTES = 2.0 ** 20.0\n",
        "                while True:\n",
        "                    data = response.read(8192)\n",
        "                    if not data:\n",
        "                        break\n",
        "                    counter += len(data)\n",
        "                    output_file.write(data)\n",
        "\n",
        "                    # We perform animation by overwriting the elements.\n",
        "                    weights_warning.warning(\n",
        "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
        "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
        "                    )\n",
        "                    progress_bar.progress(min(counter / length, 1.0))\n",
        "    # Finally, we remove these visual elements by calling .empty().\n",
        "    finally:\n",
        "        if weights_warning is not None:\n",
        "            weights_warning.empty()\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.empty()\n",
        "\n",
        "\n",
        "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
        "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
        "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
        "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
        "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
        "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt.txt\"\n",
        "\n",
        "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH, key='caffe_model_download') #, expected_size=23147564)\n",
        "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH, key='onnx_model_download') #, expected_size=23147564)\n",
        "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH,  key='prototxt_download') #, expected_size=29353)\n",
        "\n",
        "# Session-specific caching\n",
        "onnx_cache_key = \"emotion_dnn\"\n",
        "caffe_cache_key = \"face_detection_dnn\"\n",
        "if caffe_cache_key in st.session_state and onnx_cache_key in st.session_state:\n",
        "  model = st.session_state[onnx_cache_key]\n",
        "  net = st.session_state[caffe_cache_key]\n",
        "else:\n",
        "  # Read ONNX model\n",
        "  model = 'onnx_model.onnx'\n",
        "  model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "  st.session_state[onnx_cache_key] = model\n",
        "  # Read the Caffe face detector.\n",
        "  net = cv2.dnn.readNetFromCaffe(str(PROTOTXT_LOCAL_PATH), str(CAFFE_MODEL_LOCAL_PATH))\n",
        "  st.session_state[caffe_cache_key] = net\n",
        "\n",
        "\n",
        "########################################\n",
        "image_mean = np.array([127, 127, 127])\n",
        "image_std = 128.0\n",
        "iou_threshold = 0.3\n",
        "center_variance = 0.1\n",
        "size_variance = 0.2\n",
        "min_boxes = [\n",
        "  [10.0, 16.0, 24.0],\n",
        "  [32.0, 48.0],\n",
        "  [64.0, 96.0],\n",
        "  [128.0, 192.0, 256.0]\n",
        "]\n",
        "strides = [8.0, 16.0, 32.0, 64.0]\n",
        "threshold = 0.5\n",
        "\n",
        "emotion_dict = {\n",
        "      0: 'neutral',\n",
        "      1: 'happiness',\n",
        "      2: 'surprise',\n",
        "      3: 'sadness',\n",
        "      4: 'anger',\n",
        "      5: 'disgust',\n",
        "      6: 'fear'\n",
        "  }\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "def define_img_size(image_size):\n",
        "  shrinkage_list = []\n",
        "  feature_map_w_h_list = []\n",
        "  for size in image_size:\n",
        "      feature_map = [int(ceil(size / stride)) for stride in strides]\n",
        "      feature_map_w_h_list.append(feature_map)\n",
        "\n",
        "  for i in range(0, len(image_size)):\n",
        "      shrinkage_list.append(strides)\n",
        "  priors = generate_priors(\n",
        "      feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
        "  )\n",
        "  return priors\n",
        "\n",
        "\n",
        "def generate_priors(\n",
        "  feature_map_list, shrinkage_list, image_size, min_boxes\n",
        "):\n",
        "  priors = []\n",
        "  for index in range(0, len(feature_map_list[0])):\n",
        "      scale_w = image_size[0] / shrinkage_list[0][index]\n",
        "      scale_h = image_size[1] / shrinkage_list[1][index]\n",
        "      for j in range(0, feature_map_list[1][index]):\n",
        "          for i in range(0, feature_map_list[0][index]):\n",
        "              x_center = (i + 0.5) / scale_w\n",
        "              y_center = (j + 0.5) / scale_h\n",
        "\n",
        "              for min_box in min_boxes[index]:\n",
        "                  w = min_box / image_size[0]\n",
        "                  h = min_box / image_size[1]\n",
        "                  priors.append([\n",
        "                      x_center,\n",
        "                      y_center,\n",
        "                      w,\n",
        "                      h\n",
        "                  ])\n",
        "  print(\"priors nums:{}\".format(len(priors)))\n",
        "  return np.clip(priors, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
        "  scores = box_scores[:, -1]\n",
        "  boxes = box_scores[:, :-1]\n",
        "  picked = []\n",
        "  indexes = np.argsort(scores)\n",
        "  indexes = indexes[-candidate_size:]\n",
        "  while len(indexes) > 0:\n",
        "      current = indexes[-1]\n",
        "      picked.append(current)\n",
        "      if 0 < top_k == len(picked) or len(indexes) == 1:\n",
        "          break\n",
        "      current_box = boxes[current, :]\n",
        "      indexes = indexes[:-1]\n",
        "      rest_boxes = boxes[indexes, :]\n",
        "      iou = iou_of(\n",
        "          rest_boxes,\n",
        "          np.expand_dims(current_box, axis=0),\n",
        "      )\n",
        "      indexes = indexes[iou <= iou_threshold]\n",
        "  return box_scores[picked, :]\n",
        "\n",
        "\n",
        "def area_of(left_top, right_bottom):\n",
        "  hw = np.clip(right_bottom - left_top, 0.0, None)\n",
        "  return hw[..., 0] * hw[..., 1]\n",
        "\n",
        "\n",
        "def iou_of(boxes0, boxes1, eps=1e-5):\n",
        "  overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
        "  overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
        "\n",
        "  overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
        "  area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
        "  area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
        "  return overlap_area / (area0 + area1 - overlap_area + eps)\n",
        "\n",
        "\n",
        "def predict(\n",
        "  width,\n",
        "  height,\n",
        "  confidences,\n",
        "  boxes,\n",
        "  prob_threshold,\n",
        "  iou_threshold=0.3,\n",
        "  top_k=-1\n",
        "):\n",
        "  boxes = boxes[0]\n",
        "  confidences = confidences[0]\n",
        "  picked_box_probs = []\n",
        "  picked_labels = []\n",
        "  for class_index in range(1, confidences.shape[1]):\n",
        "      probs = confidences[:, class_index]\n",
        "      mask = probs > prob_threshold\n",
        "      probs = probs[mask]\n",
        "      if probs.shape[0] == 0:\n",
        "          continue\n",
        "      subset_boxes = boxes[mask, :]\n",
        "      box_probs = np.concatenate(\n",
        "          [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
        "      )\n",
        "      box_probs = hard_nms(box_probs,\n",
        "                            iou_threshold=iou_threshold,\n",
        "                            top_k=top_k,\n",
        "                            )\n",
        "      picked_box_probs.append(box_probs)\n",
        "      picked_labels.extend([class_index] * box_probs.shape[0])\n",
        "  if not picked_box_probs:\n",
        "      return np.array([]), np.array([]), np.array([])\n",
        "  picked_box_probs = np.concatenate(picked_box_probs)\n",
        "  picked_box_probs[:, 0] *= width\n",
        "  picked_box_probs[:, 1] *= height\n",
        "  picked_box_probs[:, 2] *= width\n",
        "  picked_box_probs[:, 3] *= height\n",
        "  return (\n",
        "      picked_box_probs[:, :4].astype(np.int32),\n",
        "      np.array(picked_labels),\n",
        "      picked_box_probs[:, 4]\n",
        "  )\n",
        "\n",
        "\n",
        "def convert_locations_to_boxes(locations, priors, center_variance,\n",
        "                              size_variance):\n",
        "  if len(priors.shape) + 1 == len(locations.shape):\n",
        "      priors = np.expand_dims(priors, 0)\n",
        "  return np.concatenate([\n",
        "      locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
        "      np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
        "  ], axis=len(locations.shape) - 1)\n",
        "\n",
        "\n",
        "def center_form_to_corner_form(locations):\n",
        "  return np.concatenate(\n",
        "      [locations[..., :2] - locations[..., 2:] / 2,\n",
        "        locations[..., :2] + locations[..., 2:] / 2],\n",
        "      len(locations.shape) - 1\n",
        "  )\n",
        "\n",
        "\n",
        "# def FER_live_cam():\n",
        "def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:\n",
        "  frame = frame.to_ndarray(format=\"bgr24\")\n",
        "\n",
        "  # cap = cv2.VideoCapture('video3.mp4')\n",
        "  # cap = cv2.VideoCapture(0)\n",
        "\n",
        "  # frame_width = int(cap.get(3))\n",
        "  # frame_height = int(cap.get(4))\n",
        "  # size = (frame_width, frame_height)\n",
        "  # result = cv2.VideoWriter('infer2-test.avi',\n",
        "  #                      cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "  #                      10, size)\n",
        "  # Read ONNX model\n",
        "  # model = 'onnx_model.onnx'\n",
        "  # model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "\n",
        "  # # Read the Caffe face detector.\n",
        "  # model_path = 'RFB-320/RFB-320.caffemodel'\n",
        "  # proto_path = 'RFB-320/RFB-320.prototxt'\n",
        "  # net = dnn.readNetFromCaffe(proto_path, model_path)\n",
        "\n",
        "  input_size = [320, 240]\n",
        "  width = input_size[0]\n",
        "  height = input_size[1]\n",
        "  priors = define_img_size(input_size)\n",
        "\n",
        "  # while cap.isOpened():\n",
        "  #     ret, frame = cap.read()\n",
        "  #     if ret:\n",
        "  img_ori = frame\n",
        "  #print(\"frame size: \", frame.shape)\n",
        "  rect = cv2.resize(img_ori, (width, height))\n",
        "  rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
        "  net.setInput(dnn.blobFromImage(rect, 1 / image_std, (width, height), 127))\n",
        "  start_time = time.time()\n",
        "  boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
        "  boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
        "  scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
        "  boxes = convert_locations_to_boxes(\n",
        "      boxes, priors, center_variance, size_variance\n",
        "  )\n",
        "  boxes = center_form_to_corner_form(boxes)\n",
        "  boxes, labels, probs = predict(\n",
        "      img_ori.shape[1],\n",
        "      img_ori.shape[0],\n",
        "      scores,\n",
        "      boxes,\n",
        "      threshold\n",
        "  )\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  for (x1, y1, x2, y2) in boxes:\n",
        "      w = x2 - x1\n",
        "      h = y2 - y1\n",
        "      cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
        "      resize_frame = cv2.resize(\n",
        "          gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
        "      )\n",
        "      resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
        "      model.setInput(resize_frame)\n",
        "      output = model.forward()\n",
        "      end_time = time.time()\n",
        "      fps = 1 / (end_time - start_time)\n",
        "      print(f\"FPS: {fps:.1f}\")\n",
        "      pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
        "      cv2.rectangle(\n",
        "          img_ori,\n",
        "          (x1, y1),\n",
        "          (x2, y2),\n",
        "          (215, 5, 247),\n",
        "          2,\n",
        "          lineType=cv2.LINE_AA\n",
        "      )\n",
        "      cv2.putText(\n",
        "          frame,\n",
        "          pred,\n",
        "          (x1, y1-10),\n",
        "          cv2.FONT_HERSHEY_SIMPLEX,\n",
        "          0.8,\n",
        "          (215, 5, 247),\n",
        "          2,\n",
        "          lineType=cv2.LINE_AA\n",
        "      )\n",
        "\n",
        "  #         result.write(frame)\n",
        "\n",
        "  #         cv2.imshow('frame', frame)\n",
        "  #         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "  #             break\n",
        "  #     else:\n",
        "  #         break\n",
        "\n",
        "  # cap.release()\n",
        "  # result.release()\n",
        "  # cv2.destroyAllWindows()\n",
        "  return av.VideoFrame.from_ndarray(frame, format=\"bgr24\")\n",
        "\n",
        "\n",
        "\n",
        "def get_ice_servers():\n",
        "    \"\"\"Use Twilio's TURN server because Streamlit Community Cloud has changed\n",
        "    its infrastructure and WebRTC connection cannot be established without TURN server now.  # noqa: E501\n",
        "    We considered Open Relay Project (https://www.metered.ca/tools/openrelay/) too,\n",
        "    but it is not stable and hardly works as some people reported like https://github.com/aiortc/aiortc/issues/832#issuecomment-1482420656  # noqa: E501\n",
        "    See https://github.com/whitphx/streamlit-webrtc/issues/1213\n",
        "    \"\"\"\n",
        "\n",
        "    # Ref: https://www.twilio.com/docs/stun-turn/api\n",
        "    try:\n",
        "        account_sid = os.environ[\"TWILIO_ACCOUNT_SID\"]\n",
        "        auth_token = os.environ[\"TWILIO_AUTH_TOKEN\"]\n",
        "    except KeyError:\n",
        "        print(\n",
        "            \"Twilio credentials are not set. Fallback to a free STUN server from Google.\"  # noqa: E501\n",
        "        )\n",
        "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
        "\n",
        "    client = Client(account_sid, auth_token)\n",
        "\n",
        "    try:\n",
        "        token = client.tokens.create()\n",
        "    except TwilioRestException as e:\n",
        "        st.warning(\n",
        "            f\"Error occurred while accessing Twilio API. Fallback to a free STUN server from Google. ({e})\"  # noqa: E501\n",
        "        )\n",
        "        return [{\"urls\": [\"stun:stun.l.google.com:19302\"]}]\n",
        "\n",
        "    return token.ice_servers\n",
        "\n",
        "def main():\n",
        "    st.title(\"WebRTC in Streamlit on Colab\")\n",
        "    ice_servers = get_ice_servers()\n",
        "    webrtc_ctx = webrtc_streamer(\n",
        "    key=\"object-detection\",\n",
        "    mode=WebRtcMode.SENDRECV,\n",
        "    rtc_configuration={\"iceServers\": ice_servers},\n",
        "    video_frame_callback=video_frame_callback,\n",
        "    media_stream_constraints={\"video\": True, \"audio\": False},\n",
        "    async_processing=True,\n",
        ")\n",
        "\n",
        "    if st.checkbox(\"Show the detected labels\", value=True):\n",
        "      if webrtc_ctx.state.playing:\n",
        "          labels_placeholder = st.empty()\n",
        "          # NOTE: The video transformation with object detection and\n",
        "          # this loop displaying the result labels are running\n",
        "          # in different threads asynchronously.\n",
        "          # Then the rendered video frames and the labels displayed here\n",
        "          # are not strictly synchronized.\n",
        "\n",
        "    st.markdown(\n",
        "    \"This demo uses a model and code from \"\n",
        "    \"https://github.com/spmallick/learnopncv/Facial-Emotion-Recognition. \"\n",
        "    \"Many thanks to the project.\"\n",
        "  )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaZ2RMaTQXZf"
      },
      "source": [
        "## Start ngrok Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_e5-KttQaAG",
        "outputId": "6b7b8bf5-5060-4da2-f3cb-63a04bbc4029"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "ngrok.set_auth_token('2g4zHuxbtXn6eAFHzTCbyOt4ZH4_5B1myF1riDk7YkhU5mcea')\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILsRlsqXQcY8"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/dev/null&\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKnqtv77TYOk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO/mZ5BaEiaEu6qn1kJGHXY",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
