{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nworb999/emotiscope/blob/main/emotoscope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg01WmrKT_j3"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cc5kXa335Vu",
        "outputId": "19be0ef1-b33f-4369-f6ef-e00871884263"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers transformers scipy ftfy accelerate torch torchvision np opencv-python streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wu-C63erGuw"
      },
      "source": [
        "#Hugging Face Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Au_xHFa_19o",
        "outputId": "9af8a763-07e6-4a94-d91e-99528b7d138e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Securely input the API token\n",
        "api_token = getpass.getpass(\"Enter your API token:\")\n",
        "os.environ[\"HF_TOKEN\"] = api_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd0m4JU4Uyx-"
      },
      "source": [
        "# Clear CUDA cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Jj2olJUyPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hlA7jukuIv4"
      },
      "source": [
        "# Facial Expression Recognition Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsNmAw7_vmEP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import cv2 as cv2\n",
        "import time as time\n",
        "\n",
        "image_mean = np.array([127, 127, 127])\n",
        "image_std = 128.0\n",
        "iou_threshold = 0.3\n",
        "center_variance = 0.1\n",
        "size_variance = 0.2\n",
        "min_boxes = [\n",
        "    [10.0, 16.0, 24.0],\n",
        "    [32.0, 48.0],\n",
        "    [64.0, 96.0],\n",
        "    [128.0, 192.0, 256.0]\n",
        "]\n",
        "strides = [8.0, 16.0, 32.0, 64.0]\n",
        "threshold = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5kTIbUayQTy"
      },
      "outputs": [],
      "source": [
        "def generate_priors(feature_map_list, shrinkage_list, image_size, min_boxes):\n",
        "    priors = []\n",
        "    for index in range(0, len(feature_map_list[0])):\n",
        "        scale_w = image_size[0] / shrinkage_list[0][index]\n",
        "        scale_h = image_size[1] / shrinkage_list[1][index]\n",
        "        for j in range(0, feature_map_list[1][index]):\n",
        "            for i in range(0, feature_map_list[0][index]):\n",
        "                x_center = (i + 0.5) / scale_w\n",
        "                y_center = (j + 0.5) / scale_h\n",
        "\n",
        "                for min_box in min_boxes[index]:\n",
        "                    w = min_box / image_size[0]\n",
        "                    h = min_box / image_size[1]\n",
        "                    priors.append([\n",
        "                        x_center,\n",
        "                        y_center,\n",
        "                        w,\n",
        "                        h\n",
        "                    ])\n",
        "    print(\"priors nums:{}\".format(len(priors)))\n",
        "    return np.clip(priors, 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxT-PFnbxQKZ"
      },
      "outputs": [],
      "source": [
        "def define_img_size(image_size):\n",
        "    shrinkage_list = []\n",
        "    feature_map_w_h_list = []\n",
        "    for size in image_size:\n",
        "        feature_map = [int(math.ceil(size / stride)) for stride in strides]\n",
        "        feature_map_w_h_list.append(feature_map)\n",
        "\n",
        "    for i in range(0, len(image_size)):\n",
        "        shrinkage_list.append(strides)\n",
        "    priors = generate_priors(\n",
        "        feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
        "    )\n",
        "    return priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0f9Bb3vzG9O"
      },
      "outputs": [],
      "source": [
        "def area_of(left_top, right_bottom):\n",
        "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
        "    return hw[..., 0] * hw[..., 1]\n",
        "\n",
        "\n",
        "def iou_of(boxes0, boxes1, eps=1e-5):\n",
        "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
        "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n",
        "\n",
        "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
        "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
        "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
        "    return overlap_area / (area0 + area1 - overlap_area + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsb0kCglzCSw"
      },
      "outputs": [],
      "source": [
        "def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
        "    scores = box_scores[:, -1]\n",
        "    boxes = box_scores[:, :-1]\n",
        "    picked = []\n",
        "    indexes = np.argsort(scores)\n",
        "    indexes = indexes[-candidate_size:]\n",
        "    while len(indexes) > 0:\n",
        "        current = indexes[-1]\n",
        "        picked.append(current)\n",
        "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
        "            break\n",
        "        current_box = boxes[current, :]\n",
        "        indexes = indexes[:-1]\n",
        "        rest_boxes = boxes[indexes, :]\n",
        "        iou = iou_of(\n",
        "            rest_boxes,\n",
        "            np.expand_dims(current_box, axis=0),\n",
        "        )\n",
        "        indexes = indexes[iou <= iou_threshold]\n",
        "    return box_scores[picked, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aoz9WhagzIlr"
      },
      "outputs": [],
      "source": [
        "def predict(\n",
        "    width,\n",
        "    height,\n",
        "    confidences,\n",
        "    boxes,\n",
        "    prob_threshold,\n",
        "    iou_threshold=0.3,\n",
        "    top_k=-1\n",
        "):\n",
        "    boxes = boxes[0]\n",
        "    confidences = confidences[0]\n",
        "    picked_box_probs = []\n",
        "    picked_labels = []\n",
        "    for class_index in range(1, confidences.shape[1]):\n",
        "        probs = confidences[:, class_index]\n",
        "        mask = probs > prob_threshold\n",
        "        probs = probs[mask]\n",
        "        if probs.shape[0] == 0:\n",
        "            continue\n",
        "        subset_boxes = boxes[mask, :]\n",
        "        box_probs = np.concatenate(\n",
        "            [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
        "        )\n",
        "        box_probs = hard_nms(box_probs,\n",
        "                             iou_threshold=iou_threshold,\n",
        "                             top_k=top_k,\n",
        "                             )\n",
        "        picked_box_probs.append(box_probs)\n",
        "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
        "    if not picked_box_probs:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    picked_box_probs = np.concatenate(picked_box_probs)\n",
        "    picked_box_probs[:, 0] *= width\n",
        "    picked_box_probs[:, 1] *= height\n",
        "    picked_box_probs[:, 2] *= width\n",
        "    picked_box_probs[:, 3] *= height\n",
        "    return (\n",
        "        picked_box_probs[:, :4].astype(np.int32),\n",
        "        np.array(picked_labels),\n",
        "        picked_box_probs[:, 4]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCCHOR7cz_w6"
      },
      "outputs": [],
      "source": [
        "def convert_locations_to_boxes(locations, priors, center_variance,\n",
        "                               size_variance):\n",
        "    if len(priors.shape) + 1 == len(locations.shape):\n",
        "        priors = np.expand_dims(priors, 0)\n",
        "    return np.concatenate([\n",
        "        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
        "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
        "    ], axis=len(locations.shape) - 1)\n",
        "\n",
        "\n",
        "def center_form_to_corner_form(locations):\n",
        "    return np.concatenate(\n",
        "        [locations[..., :2] - locations[..., 2:] / 2,\n",
        "         locations[..., :2] + locations[..., 2:] / 2],\n",
        "        len(locations.shape) - 1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7wkzsfP8sp2"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# This code is based on https://github.com/streamlit/demo-self-driving/blob/230245391f2dda0cb464008195a470751c01770b/streamlit_app.py#L48  # noqa: E501\n",
        "# and I, Emma, ripped this from https://huggingface.co/spaces/sadimanna/face-emotion-recognition/\n",
        "\n",
        "def download_file(url, download_to: Path, expected_size=None):\n",
        "    # Don't download the file twice.\n",
        "    # (If possible, verify the download using the file length.)\n",
        "    if download_to.exists():\n",
        "        if expected_size:\n",
        "            if download_to.stat().st_size == expected_size:\n",
        "                return\n",
        "        else:\n",
        "            st.info(f\"{url} is already downloaded.\")\n",
        "            if not st.button(\"Download again?\"):\n",
        "                return\n",
        "\n",
        "    download_to.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # These are handles to two visual elements to animate.\n",
        "    weights_warning, progress_bar = None, None\n",
        "    try:\n",
        "        weights_warning = st.warning(\"Downloading %s...\" % url)\n",
        "        progress_bar = st.progress(0)\n",
        "        with open(download_to, \"wb\") as output_file:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                length = int(response.info()[\"Content-Length\"])\n",
        "                counter = 0.0\n",
        "                MEGABYTES = 2.0 ** 20.0\n",
        "                while True:\n",
        "                    data = response.read(8192)\n",
        "                    if not data:\n",
        "                        break\n",
        "                    counter += len(data)\n",
        "                    output_file.write(data)\n",
        "\n",
        "                    # We perform animation by overwriting the elements.\n",
        "                    weights_warning.warning(\n",
        "                        \"Downloading %s... (%6.2f/%6.2f MB)\"\n",
        "                        % (url, counter / MEGABYTES, length / MEGABYTES)\n",
        "                    )\n",
        "                    progress_bar.progress(min(counter / length, 1.0))\n",
        "    # Finally, we remove these visual elements by calling .empty().\n",
        "    finally:\n",
        "        if weights_warning is not None:\n",
        "            weights_warning.empty()\n",
        "        if progress_bar is not None:\n",
        "            progress_bar.empty()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifyIFtvm8ZF4"
      },
      "outputs": [],
      "source": [
        "HERE = Path.cwd()\n",
        "ROOT = HERE\n",
        "\n",
        "ONNX_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/emotion-ferplus-8.onnx\"\n",
        "ONNX_MODEL_LOCAL_PATH = ROOT / \"./emotion-ferplus-8.onnx\"\n",
        "CAFFE_MODEL_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.caffemodel\"  # noqa: E501\n",
        "CAFFE_MODEL_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.caffemodel\"\n",
        "PROTOTXT_URL = \"https://github.com/spmallick/learnopencv/raw/master/Facial-Emotion-Recognition/RFB-320/RFB-320.prototxt\"  # noqa: E501\n",
        "PROTOTXT_LOCAL_PATH = ROOT / \"./RFB-320/RFB-320.prototxt\"\n",
        "\n",
        "download_file(CAFFE_MODEL_URL, CAFFE_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(ONNX_MODEL_URL, ONNX_MODEL_LOCAL_PATH) #, expected_size=23147564)\n",
        "download_file(PROTOTXT_URL, PROTOTXT_LOCAL_PATH) #, expected_size=29353)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MB03ztJZXp"
      },
      "source": [
        "# Live Camera Capture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqFBfGfu0iYI"
      },
      "outputs": [],
      "source": [
        "def FER_live_cam():\n",
        "    emotion_dict = {\n",
        "        0: 'neutral',\n",
        "        1: 'happiness',\n",
        "        2: 'surprise',\n",
        "        3: 'sadness',\n",
        "        4: 'anger',\n",
        "        5: 'disgust',\n",
        "        6: 'fear'\n",
        "    }\n",
        "\n",
        "    # cap = cv2.VideoCapture('video1.mp4')\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    size = (frame_width, frame_height)\n",
        "    result = cv2.VideoWriter('result.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         10, size)\n",
        "\n",
        "    # Read ONNX model\n",
        "    # model = 'onnx_model.onnx'\n",
        "    model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
        "\n",
        "    # Read the Caffe face detector.\n",
        "    model_path = 'RFB-320/RFB-320.caffemodel'\n",
        "    proto_path = 'RFB-320/RFB-320.prototxt'\n",
        "    net = cv2.dnn.readNetFromCaffe(proto_path, model_path)\n",
        "    input_size = [320, 240]\n",
        "    width = input_size[0]\n",
        "    height = input_size[1]\n",
        "    priors = define_img_size(input_size)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            img_ori = frame\n",
        "            #print(\"frame size: \", frame.shape)\n",
        "            rect = cv2.resize(img_ori, (width, height))\n",
        "            rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
        "            net.setInput(cv2.dnn.blobFromImage(\n",
        "                rect, 1 / image_std, (width, height), 127)\n",
        "            )\n",
        "            start_time = time.time()\n",
        "            boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
        "            boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
        "            scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
        "            boxes = convert_locations_to_boxes(\n",
        "                boxes, priors, center_variance, size_variance\n",
        "            )\n",
        "            boxes = center_form_to_corner_form(boxes)\n",
        "            boxes, labels, probs = predict(\n",
        "                img_ori.shape[1],\n",
        "                img_ori.shape[0],\n",
        "                scores,\n",
        "                boxes,\n",
        "                threshold\n",
        "            )\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            for (x1, y1, x2, y2) in boxes:\n",
        "                w = x2 - x1\n",
        "                h = y2 - y1\n",
        "                cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
        "                resize_frame = cv2.resize(\n",
        "                    gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
        "                )\n",
        "                resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
        "                model.setInput(resize_frame)\n",
        "                output = model.forward()\n",
        "                end_time = time.time()\n",
        "                fps = 1 / (end_time - start_time)\n",
        "                print(f\"FPS: {fps:.1f}\")\n",
        "                pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
        "                cv2.rectangle(\n",
        "                    img_ori,\n",
        "                    (x1, y1),\n",
        "                    (x2, y2),\n",
        "                    (0, 255, 0),\n",
        "                    2,\n",
        "                    lineType=cv2.LINE_AA\n",
        "                )\n",
        "                cv2.putText(\n",
        "                    frame,\n",
        "                    pred,\n",
        "                    (x1, y1),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8,\n",
        "                    (0, 255, 0),\n",
        "                    2,\n",
        "                    lineType=cv2.LINE_AA\n",
        "                )\n",
        "\n",
        "            result.write(frame)\n",
        "\n",
        "            cv2.imshow('frame', frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    result.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obOoWp7q01et",
        "outputId": "95e4f810-24da-4e7a-954f-56364267c58e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Lv_izc7vfZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNDWWzZeRKWSi/jV2/CKU6N",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
